{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp311-cp311-win_amd64.whl (7.8 MB)\n",
      "     ---------------------------------------- 7.8/7.8 MB 23.8 MB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl (219 kB)\n",
      "     ------------------------------------- 219.8/219.8 kB 13.1 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.55.0-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 23.5 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.0/56.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prass\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-11.0.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 23.5 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "     -------------------------------------- 106.9/106.9 kB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\prass\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prass\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.0 kiwisolver-1.4.7 matplotlib-3.9.2 pillow-11.0.0 pyparsing-3.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prass\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prass\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prass\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch\n",
    "!python -m pip install matplotlib\n",
    "!python -m pip install numpy\n",
    "!python -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (5): ReLU()\n",
      ")\n",
      "0 [1, 3]\n",
      "tensor([0., 0., 0., 0.])\n",
      "2 [1, 3]\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "1 [0, 2]\n",
      "tensor([0., 0., 0., 0.])\n",
      "3 [0, 2]\n",
      "0 [1, 3]\n",
      "tensor([0., 0., 0., 0.])\n",
      "2 [1, 3]\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "1 [0, 2]\n",
      "tensor([0., 0., 0., 0.])\n",
      "3 [0, 2]\n",
      "tensor([[0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SparseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, h, connections = None):\n",
    "        super(SparseNeuralNetwork, self).__init__()\n",
    "        h = [1] + h\n",
    "        self.univariate_nn = nn.Sequential()\n",
    "        layers = []\n",
    "        self.masks = []\n",
    "        for layer in range(1,len(h)):\n",
    "            layers.append(nn.Linear(h[layer -1] * in_dim, h[layer] * in_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            self.masks.append(self.hidden_sparistiy_masks(h[layer] * in_dim, h[layer -1] * in_dim, h[layer - 1],h[layer]))\n",
    "        self.univariate_nn = nn.Sequential(*layers)\n",
    "        print(self.univariate_nn)\n",
    "        self.multiply_weight_masks()\n",
    "        self.fc2 = nn.Linear(out_dim, h[-1] * in_dim)\n",
    "        if connections is not None:\n",
    "            self.connection_mask = self.get_connection_mask(h[-1], in_dim, out_dim, connections)\n",
    "\n",
    "    def multiply_connection_weight_masks(self):\n",
    "        with torch.no_grad():\n",
    "            for i in range(0,len(self.univariate_nn),2):\n",
    "                self.fc2.weight.mul_(self.connection_mask)\n",
    "\n",
    "    def multiply_grad_weight_masks(self):\n",
    "        with torch.no_grad():\n",
    "            for i in range(0,len(self.univariate_nn),2):\n",
    "                self.fc2.weight.grad.mul_(self.connection_mask)\n",
    "\n",
    "    def get_connection_mask(self, h, in_dim, out_dim, connections):\n",
    "        mask = torch.zeros(out_dim, h * in_dim)\n",
    "        for i in range(in_dim):\n",
    "            for j in range(out_dim):\n",
    "                if j in connections[i]:\n",
    "                    print(mask[j,i * h:(i+1) * h])\n",
    "                    mask[j,i * h:(i+1)* h] = 1\n",
    "                else:\n",
    "                    print(j, connections[i])\n",
    "        print(mask)\n",
    "        return mask\n",
    "\n",
    "    def multiply_weight_masks(self):\n",
    "        with torch.no_grad():\n",
    "            for i in range(0,len(self.univariate_nn),2):\n",
    "                self.univariate_nn[i].weight.mul_(self.masks[i // 2])\n",
    "\n",
    "    def multiply_grad_masks(self):\n",
    "        with torch.no_grad():\n",
    "            for i in range(0,len(self.univariate_nn),2):\n",
    "                self.univariate_nn[i].weight.grad.mul_(self.masks[i // 2])\n",
    "\n",
    "    def hidden_sparistiy_masks(self, out_dim, in_dim, input_neurons, output_neurons):\n",
    "        mask = torch.zeros(out_dim, in_dim)\n",
    "        for i in range(0,in_dim):\n",
    "            mask[i*output_neurons:output_neurons*(i + 1) , i*input_neurons:(i + 1)*input_neurons] = 1\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.univariate_nn(x)\n",
    "        output = self.fc2(hidden)\n",
    "        return output\n",
    "h = 3 \n",
    "in_dim = 5  \n",
    "out_dim = 1  \n",
    "#model = SparseNeuralNetwork(in_dim, out_dim, [4,8,4])\n",
    "model = SparseNeuralNetwork(4, 4, [4,8,4], connections = [[1,3],[0,2],[1,3],[0,2]])\n",
    "#input_data = torch.ones(1, in_dim)\n",
    "#output = model(input_data)\n",
    "#print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "1 1\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000020FB3CDD490>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x64 and 1x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\Neural_KAN.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m dataloader \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_dataloader(f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(dataloader)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(dataloader \u001b[39m=\u001b[39;49m dataloader, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\Neural_KAN.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\Neural_KAN.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\Neural_KAN.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munivariate_nn(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Neural_KAN.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x64 and 1x64)"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Neural_Kan(nn.Module):\n",
    "    def __init__(self, shape, h):\n",
    "        super(Neural_Kan, self).__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        for i in range(len(shape) - 1):\n",
    "            print(shape[i], shape[i + 1])\n",
    "            self.layers.append(SparseNeuralNetwork(in_dim = shape[i], out_dim = shape[i + 1], h = h))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def train_loss(self,dataloader):\n",
    "        criterion = nn.MSELoss()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in dataloader:\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss\n",
    "\n",
    "    def fit(self,dataloader, dataloader_test = '', epochs=100, lr=0.001):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        self.train()\n",
    "        loss_list = []\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for inputs, targets in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                for models in self.layers:\n",
    "                    models.multiply_grad_masks()\n",
    "                optimizer.step()\n",
    "            #test_loss = self.train_loss(dataloader_test)\n",
    "            loss_list.append(total_loss)\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            #avg_testloss = test_loss / len(dataloader_test)\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.7f})#, Test_loss: {avg_testloss:.7f} \")\n",
    "            plt.plot(loss_list)\n",
    "\n",
    "    def get_dataloader(self,f,num_samples=1000, in_dim=2, out_dim=1, batch_size = 32):\n",
    "        X = torch.rand(num_samples, in_dim) \n",
    "        train_dataset = torch.utils.data.TensorDataset(X, f(X))\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "        return train_dataloader\n",
    "\n",
    "model = Neural_Kan(shape = [2,1,1], h = [32])\n",
    "def f(X):\n",
    "    return torch.sin(torch.sum(X, dim=1, keepdim=True))\n",
    "dataloader = model.get_dataloader(f)\n",
    "print(dataloader)\n",
    "model.fit(dataloader = dataloader, epochs=100, lr=0.001)\n",
    "        \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nomograph(nn.Module):\n",
    "    def __init__(self,in_dim, out_dim = 1 ,h = 32):\n",
    "        super(Nomograph, self).__init__()\n",
    "        self.inner = SparseNeuralNetwork(in_dim, out_dim, h)\n",
    "        self.outer_hidden = nn.Linear(out_dim,h)\n",
    "        self.outer = nn.Linear(h,out_dim)\n",
    "    def forward(self, x):\n",
    "        #print(self.inner.fc1.weight.shape)\n",
    "        #return self.inner(x)\n",
    "        inner = F.relu(self.inner(x))\n",
    "        outer_hidden = F.relu(self.outer_hidden(inner))\n",
    "        return self.outer(outer_hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (5): ReLU()\n",
      ")\n",
      "2 1\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (5): ReLU()\n",
      ")\n",
      "Epoch [1/10], Loss: 0.5242\n",
      "Epoch [2/10], Loss: 0.5056\n",
      "Epoch [3/10], Loss: 0.4403\n",
      "Epoch [4/10], Loss: 0.2088\n",
      "Epoch [5/10], Loss: 0.1064\n",
      "Epoch [6/10], Loss: 0.0808\n",
      "Epoch [7/10], Loss: 0.0630\n",
      "Epoch [8/10], Loss: 0.0463\n",
      "Epoch [9/10], Loss: 0.0339\n",
      "Epoch [10/10], Loss: 0.0240\n",
      "Parameter containing:\n",
      "tensor([[-0.0579,  0.0000],\n",
      "        [-0.4264,  0.0000],\n",
      "        [ 0.3061, -0.0000],\n",
      "        [-0.5274, -0.0000],\n",
      "        [ 0.8299, -0.0000],\n",
      "        [-0.8010, -0.0000],\n",
      "        [-0.4479,  0.0000],\n",
      "        [ 0.3067, -0.0000],\n",
      "        [-0.0000, -0.1540],\n",
      "        [ 0.0000, -0.2693],\n",
      "        [ 0.0000,  0.7049],\n",
      "        [ 0.0000,  0.4701],\n",
      "        [-0.0000,  0.2737],\n",
      "        [-0.0000, -0.3374],\n",
      "        [-0.0000,  0.1293],\n",
      "        [-0.0000, -0.3067]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0568,  0.2003,  0.0428,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.1839,  0.2707,  0.1545,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        [-0.1598,  0.1102,  0.0072,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        ...,\n",
      "        [-0.0000,  0.0000, -0.0000,  ..., -0.0859, -0.0887, -0.2432],\n",
      "        [-0.0000, -0.0000, -0.0000,  ..., -0.2482,  0.0713, -0.2016],\n",
      "        [-0.0000, -0.0000, -0.0000,  ..., -0.1204,  0.1207,  0.1136]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0751,  0.0313, -0.1243,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0068,  0.0397,  0.1327,  ...,  0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0043, -0.1716,  0.0154,  ..., -0.0000,  0.0000, -0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000, -0.0000,  ..., -0.0716,  0.0729, -0.0520],\n",
      "        [ 0.0000, -0.0000, -0.0000,  ...,  0.0564,  0.0771, -0.0547],\n",
      "        [-0.0000, -0.0000,  0.0000,  ..., -0.0180,  0.0563, -0.0042]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1643, -0.0000],\n",
      "        [ 0.4830,  0.0000],\n",
      "        [ 0.8237,  0.0000],\n",
      "        [ 0.1581,  0.0000],\n",
      "        [-0.2189,  0.0000],\n",
      "        [-0.8167, -0.0000],\n",
      "        [ 0.6538,  0.0000],\n",
      "        [-0.5132, -0.0000],\n",
      "        [ 0.0000,  0.4321],\n",
      "        [ 0.0000,  0.6122],\n",
      "        [ 0.0000, -0.5490],\n",
      "        [-0.0000, -0.1467],\n",
      "        [ 0.0000,  0.6963],\n",
      "        [-0.0000,  0.7038],\n",
      "        [ 0.0000,  0.5780],\n",
      "        [-0.0000, -0.1568]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1489, -0.1511, -0.1348,  ..., -0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0452,  0.1379, -0.1688,  ..., -0.0000,  0.0000, -0.0000],\n",
      "        [-0.0932,  0.2360,  0.1396,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        ...,\n",
      "        [ 0.0000, -0.0000,  0.0000,  ...,  0.2326, -0.1234,  0.1389],\n",
      "        [-0.0000,  0.0000, -0.0000,  ...,  0.0206,  0.3254,  0.2434],\n",
      "        [ 0.0000,  0.0000, -0.0000,  ..., -0.0167,  0.0688, -0.1987]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2221, -0.0645, -0.1875,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0333, -0.0132, -0.1019,  ...,  0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0663, -0.1212, -0.2115,  ..., -0.0000, -0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000, -0.0000, -0.0000,  ..., -0.1352,  0.1047, -0.1125],\n",
      "        [-0.0000,  0.0000, -0.0000,  ..., -0.0486, -0.3250,  0.0597],\n",
      "        [-0.0000, -0.0000,  0.0000,  ...,  0.1180, -0.2865,  0.0492]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, dataloader, num_epochs=100, lr=0.001):\n",
    "    # Optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            for models in model.layers:\n",
    "                models.multiply_grad_masks()\n",
    "            optimizer.step()\n",
    "        loss_list.append(total_loss)\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "        plt.plot(total_loss)\n",
    "\n",
    "\n",
    "def generate_synthetic_data(num_samples=1000, in_dim=10, out_dim=1):\n",
    "    # For simplicity, we'll generate some random data and create a linear relationship\n",
    "    X = torch.randn(num_samples, in_dim)  # Random input data\n",
    "    y = torch.sin(torch.sum(X, dim=1, keepdim=True))  # Simple sum across the input as a target\n",
    "\n",
    "    # Let's assume our function is mapping input to some output (nomographic style)\n",
    "    return X, y\n",
    "\n",
    "in_dim = 2  \n",
    "out_dim = 1\n",
    "h = [8,32,8]\n",
    "num_epochs = 10 \n",
    "batch_size = 32\n",
    "lr = 0.001  \n",
    "X_train, y_train = generate_synthetic_data(num_samples=1000, in_dim=in_dim, out_dim=out_dim)\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "model = Neural_Kan(shape = [2,2,1], h = h)\n",
    "train(model, train_dataloader, num_epochs=num_epochs, lr=lr)\n",
    "with torch.no_grad():\n",
    "    for models in model.layers:\n",
    "        for module in models.univariate_nn:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                print(module.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5])\n",
      "torch.Size([10, 1, 50])\n",
      "torch.Size([10, 5])\n",
      "torch.Size([10, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=True):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(input_size, self.hidden_size, num_layers = 1)\n",
    "        \n",
    "        # Fully connected layer to map the hidden state to output\n",
    "        self.fc = nn.Linear(self.hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(1)\n",
    "        x.size(2)\n",
    "        hidden_state = torch.ones(1,batch_size, self.hidden_size)#num_layer, batch_size, hidden_dim\n",
    "        iters = 10\n",
    "        print(x.shape)\n",
    "        out,h = self.gru(x, hidden_state)\n",
    "        print(out.shape)\n",
    "        print(self.fc(out[:, -1, :]).shape)\n",
    "        for t in range(iters):\n",
    "            out, hidden_state = self.gru(x, hidden_state)\n",
    "            x = self.fc(out[:, -1, :])\n",
    "            x = x.reshape(-1,1,input_size) \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_size = 5  # Input dimension\n",
    "hidden_size = 10  # GRU hidden size\n",
    "\n",
    "# Create the model\n",
    "model = GRU(input_size, input_size*hidden_size)\n",
    "\n",
    "# Example input (batch_size=10, seq_length=1, input_size=5)\n",
    "x = torch.randn(10,1,input_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(output.shape)  # Expected output shape: (batch_size, seq_length, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 5])\n",
      "torch.Size([150, 50])\n",
      "torch.Size([150])\n",
      "torch.Size([150])\n",
      "torch.Size([150, 5])\n"
     ]
    }
   ],
   "source": [
    "for weight in model.gru.all_weights[0]:\n",
    "    print(weight.shape)\n",
    "\n",
    "def hidden_sparistiy_masks(out_dim=50, in_dim=5, input_neurons=1, output_neurons=10):\n",
    "    mask = torch.zeros(out_dim, in_dim)\n",
    "    for i in range(0,in_dim):\n",
    "        mask[i*output_neurons:output_neurons*(i + 1) , i*input_neurons:(i + 1)*input_neurons] = 1\n",
    "    mask = torch.vstack([mask for i in range(3)])\n",
    "    return mask\n",
    "print(hidden_sparistiy_masks().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 10])\n",
      "torch.Size([60, 20])\n",
      "torch.Size([60])\n",
      "torch.Size([60])\n",
      "tensor([[[-1.1765, -0.6020,  0.2905, -0.8836, -0.5612,  0.3081,  0.4284,\n",
      "          -0.0191,  0.0199, -0.6109, -0.1067,  0.4155,  0.3936, -0.9498,\n",
      "           1.8194,  0.5143,  1.2690, -1.0503,  0.2242,  0.1080]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "rnn = nn.GRU(10, 20, 1)\n",
    "for param in rnn.all_weights[0]:\n",
    "    print(param.shape)\n",
    "x = torch.randn(1,1,10)\n",
    "h0 = torch.randn(1,1, 20)\n",
    "output, hn = rnn(x, h0)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
