{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=True):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(input_size, self.hidden_size, num_layers = 1)\n",
    "        \n",
    "        # Fully connected layer to map the hidden state to output\n",
    "        self.fc = nn.Linear(self.hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(1)\n",
    "        x.size(2)\n",
    "        hidden_state = torch.ones(1,batch_size, self.hidden_size)#num_layer, batch_size, hidden_dim\n",
    "        iters = 10\n",
    "        print(x.shape)\n",
    "        out,h = self.gru(x, hidden_state)\n",
    "        print(out.shape)\n",
    "        print(self.fc(out[:, -1, :]).shape)\n",
    "        for t in range(iters):\n",
    "            out, hidden_state = self.gru(x, hidden_state)\n",
    "            x = self.fc(out[:, -1, :])\n",
    "            x = x.reshape(-1,1,input_size) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Lipschitz_Linear(nn.Module):\n",
    "    def __init__(self, h, activation = None):\n",
    "        super(Lipschitz_Linear, self).__init__()\n",
    "        self.B = 2 ## Upper bound on product of weights norms\n",
    "        self.lip_reg = 0.0005 ## learning rate on Lip regularisation !\n",
    "        self.order = float('inf') ### order of L_j norm\n",
    "        h = h\n",
    "        self.activation = activation\n",
    "        layers = []\n",
    "        self.linear_layers = []\n",
    "        self.Norm_constraints = torch.rand(len(h) - 1) * self.B\n",
    "        self.grads_norm_constraints = []\n",
    "        for layer in range(1,len(h)):\n",
    "            linear = nn.Linear(h[layer -1], h[layer])\n",
    "            layers.append(linear)\n",
    "            layers.append(nn.BatchNorm1d(h[layer], affine=True))\n",
    "            self.linear_layers.append(linear)\n",
    "            if activation is not None:\n",
    "                layers.append(self.activation())\n",
    "        self.univariate_nn = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.univariate_nn(x)\n",
    "    \n",
    "    def compute_constraint_gradient(self):\n",
    "        prod = torch.prod(self.Norm_constraints)\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            grad = torch.sum(self.linear_layers[i].weight.grad * (self.linear_layers[i].weight.data / (torch.linalg.matrix_norm(self.linear_layers[i].weight.data, ord = self.order)))) + self.lip_reg * (prod / self.Norm_constraints[i])  * torch.exp(7 * ((prod) - self.B))\n",
    "            self.grads_norm_constraints.append(grad)\n",
    "\n",
    "    def upper_lipschitz_bound(self):\n",
    "        return torch.prod(self.Norm_constraints)\n",
    "\n",
    "    def update_norm_constraints(self):\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            self.Norm_constraints[i] -= 0.001 * self.grads_norm_constraints[i]\n",
    "    \n",
    "    def project_on_norm_constraints(self):\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            self.linear_layers[i].weight.data *= self.Norm_constraints[i] / torch.linalg.matrix_norm(self.linear_layers[i].weight.data, ord = self.order)\n",
    "    \n",
    "    def train_enforce_constraints(self):\n",
    "        self.compute_constraint_gradient()\n",
    "        self.update_norm_constraints()\n",
    "        self.project_on_norm_constraints()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.3302\n",
      "Epoch [2/1000], Loss: 0.2347\n",
      "Epoch [3/1000], Loss: 0.2239\n",
      "Epoch [4/1000], Loss: 0.2032\n",
      "Epoch [5/1000], Loss: 0.1803\n",
      "Epoch [6/1000], Loss: 0.1580\n",
      "Epoch [7/1000], Loss: 0.1432\n",
      "Epoch [8/1000], Loss: 0.1309\n",
      "Epoch [9/1000], Loss: 0.1215\n",
      "Epoch [10/1000], Loss: 0.1121\n",
      "Epoch [11/1000], Loss: 0.1058\n",
      "Epoch [12/1000], Loss: 0.1012\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\recursive_KAN.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m outputs \u001b[39m=\u001b[39m test(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m test\u001b[39m.\u001b[39mtrain_enforce_constraints()\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "test = Lipschitz_Linear([1,4,16,4,1], activation = nn.ReLU)\n",
    "def f(X):\n",
    "    return X**0.23\n",
    "X = torch.vstack((torch.rand(1000, 1), torch.zeros(1000 //20 ,1)))\n",
    "train_dataset = torch.utils.data.TensorDataset(X, f(X))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "epochs = 1000  # Number of epochs to train\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.RAdam(test.parameters(), lr=0.001)\n",
    "for epoch in range(epochs):\n",
    "    test.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = test(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        test.train_enforce_constraints()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "print(\"Training Complete!\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3370)\n"
     ]
    }
   ],
   "source": [
    "test.upper_lipschitz_bound()\n",
    "x = 1\n",
    "for layers in test.linear_layers:\n",
    "    x *= torch.linalg.matrix_norm(layers.weight.data, ord = float('inf'))\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super(NN, self).__init__()\n",
    "        h = [1] + h\n",
    "        layers = []\n",
    "        for layer in range(1,len(h)):\n",
    "            layers.append(nn.Linear(h[layer -1], h[layer]))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.univariate_nn = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.univariate_nn(x)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAN_NN_Layer(nn.Module):\n",
    "    def __init__(self, in_dim, h, out_dim):\n",
    "        super(KAN_NN_Layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.Network_stack = []\n",
    "        for _ in range(self.in_dim):\n",
    "            #self.Network_stack.append(NN(h))\n",
    "            self.Network_stack.append(Lipschitz_Linear([1] + h, activation = nn.ReLU))\n",
    "\n",
    "        #self.fc2 = nn.Linear(in_dim * h[-1], out_dim)\n",
    "        self.fc2 = Lipschitz_Linear([in_dim * h[-1], out_dim])\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_list = []\n",
    "        for i in range(self.in_dim):\n",
    "            output_list.append(self.Network_stack[i](x[:,i].reshape(-1,1)))\n",
    "        #print(torch.cat(output_list, dim=1).shape)\n",
    "        return self.fc2(torch.cat(output_list, dim=1))\n",
    "\n",
    "    def train_enforce_constraints(self):\n",
    "        for Lip_lin in self.Network_stack:\n",
    "            Lip_lin.train_enforce_constraints()\n",
    "        self.fc2.train_enforce_constraints()\n",
    "\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAN_NN(nn.Module):\n",
    "    def __init__(self, h, hidden):\n",
    "        super(KAN_NN, self).__init__()\n",
    "        self.layers = []\n",
    "        for layer in range(1,len(h)):\n",
    "            self.layers.append(KAN_NN_Layer(h[layer -1],hidden,h[layer]))\n",
    "        self.Kan_nn = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.Kan_nn(x)\n",
    "    \n",
    "    def train_enforce_constraints(self):\n",
    "        for Lip_NN in self.layers:\n",
    "            Lip_NN.train_enforce_constraints()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 3\n",
    "model = KAN_NN([in_dim,4,2,1], [32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of KAN_NN(\n",
       "  (Kan_nn): Sequential(\n",
       "    (0): KAN_NN_Layer(\n",
       "      (fc2): Lipschitz_Linear(\n",
       "        (univariate_nn): Sequential(\n",
       "          (0): Linear(in_features=96, out_features=4, bias=True)\n",
       "          (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): KAN_NN_Layer(\n",
       "      (fc2): Lipschitz_Linear(\n",
       "        (univariate_nn): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=2, bias=True)\n",
       "          (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): KAN_NN_Layer(\n",
       "      (fc2): Lipschitz_Linear(\n",
       "        (univariate_nn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # Fixed exponents between 0 and 1\n",
    "    alpha = 0.5\n",
    "    beta = 0.7\n",
    "    gamma = 0.3\n",
    "    delta = 0.8\n",
    "    epsilon = 0.6\n",
    "    omega = 0.4\n",
    "    \n",
    "    # First term: (x^alpha + (1 - x)^beta)^gamma\n",
    "    term1 = 10*torch.sum(x ** alpha, dim = 1)\n",
    "    term2 = 2*torch.sum((1.0001 - x) ** beta, dim = 1)\n",
    "    # Second term: (sin(2Ï€x)^delta)^epsilon\n",
    "    term3 = torch.abs(torch.sin(2 * torch.pi * torch.sum(x**delta, dim = 1)))\n",
    "    term4= torch.sum(x**0.3, dim = 1)\n",
    "    result = (torch.abs((torch.sin(2 * torch.pi * (term1 ** .5)) + torch.cos(20 * torch.pi * (term2 ** .4))))**omega + torch.abs((term3**.67 + term4**0.1)))\n",
    "    return torch.reshape(result, [result.shape[0], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(100, in_dim) \n",
    "train_dataset = torch.utils.data.TensorDataset(X, f(X))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "X = torch.rand(100, in_dim)\n",
    "test_dataset = torch.utils.data.TensorDataset(X, f(X))\n",
    "test_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 7.9630, Testloss: 7.5381\n",
      "Epoch [2/500], Loss: 7.7098, Testloss: 8.2303\n",
      "Epoch [3/500], Loss: 7.4541, Testloss: 8.1184\n",
      "Epoch [4/500], Loss: 7.2342, Testloss: 8.2196\n",
      "Epoch [5/500], Loss: 7.0713, Testloss: 7.8261\n",
      "Epoch [6/500], Loss: 6.8796, Testloss: 7.5990\n",
      "Epoch [7/500], Loss: 6.6329, Testloss: 7.3763\n",
      "Epoch [8/500], Loss: 6.4687, Testloss: 6.9983\n",
      "Epoch [9/500], Loss: 6.2674, Testloss: 6.8547\n",
      "Epoch [10/500], Loss: 6.0626, Testloss: 6.8409\n",
      "Epoch [11/500], Loss: 5.8930, Testloss: 6.5229\n",
      "Epoch [12/500], Loss: 5.7353, Testloss: 6.2999\n",
      "Epoch [13/500], Loss: 5.5570, Testloss: 6.0245\n",
      "Epoch [14/500], Loss: 5.4096, Testloss: 5.7280\n",
      "Epoch [15/500], Loss: 5.2469, Testloss: 5.9099\n",
      "Epoch [16/500], Loss: 5.1240, Testloss: 5.5506\n",
      "Epoch [17/500], Loss: 4.9420, Testloss: 5.1785\n",
      "Epoch [18/500], Loss: 4.8175, Testloss: 5.2065\n",
      "Epoch [19/500], Loss: 4.6631, Testloss: 4.9323\n",
      "Epoch [20/500], Loss: 4.5574, Testloss: 4.8680\n",
      "Epoch [21/500], Loss: 4.4214, Testloss: 4.5858\n",
      "Epoch [22/500], Loss: 4.2933, Testloss: 4.5367\n",
      "Epoch [23/500], Loss: 4.1966, Testloss: 4.4183\n",
      "Epoch [24/500], Loss: 4.0230, Testloss: 4.2188\n",
      "Epoch [25/500], Loss: 3.9010, Testloss: 4.2060\n",
      "Epoch [26/500], Loss: 3.8041, Testloss: 4.1296\n",
      "Epoch [27/500], Loss: 3.6887, Testloss: 3.9435\n",
      "Epoch [28/500], Loss: 3.5607, Testloss: 3.7920\n",
      "Epoch [29/500], Loss: 3.4827, Testloss: 3.6803\n",
      "Epoch [30/500], Loss: 3.3638, Testloss: 3.5729\n",
      "Epoch [31/500], Loss: 3.2690, Testloss: 3.4553\n",
      "Epoch [32/500], Loss: 3.1596, Testloss: 3.3901\n",
      "Epoch [33/500], Loss: 3.0490, Testloss: 3.2900\n",
      "Epoch [34/500], Loss: 2.9825, Testloss: 3.1972\n",
      "Epoch [35/500], Loss: 2.9176, Testloss: 3.1277\n",
      "Epoch [36/500], Loss: 2.8239, Testloss: 2.9740\n",
      "Epoch [37/500], Loss: 2.7111, Testloss: 2.9288\n",
      "Epoch [38/500], Loss: 2.6637, Testloss: 2.8384\n",
      "Epoch [39/500], Loss: 2.5733, Testloss: 2.7376\n",
      "Epoch [40/500], Loss: 2.4906, Testloss: 2.6440\n",
      "Epoch [41/500], Loss: 2.4312, Testloss: 2.5589\n",
      "Epoch [42/500], Loss: 2.3381, Testloss: 2.4838\n",
      "Epoch [43/500], Loss: 2.2516, Testloss: 2.4006\n",
      "Epoch [44/500], Loss: 2.2184, Testloss: 2.3369\n",
      "Epoch [45/500], Loss: 2.1355, Testloss: 2.2659\n",
      "Epoch [46/500], Loss: 2.0523, Testloss: 2.1882\n",
      "Epoch [47/500], Loss: 1.9846, Testloss: 2.1134\n",
      "Epoch [48/500], Loss: 1.9303, Testloss: 2.0523\n",
      "Epoch [49/500], Loss: 1.8804, Testloss: 1.9901\n",
      "Epoch [50/500], Loss: 1.8017, Testloss: 1.9338\n",
      "Epoch [51/500], Loss: 1.7590, Testloss: 1.8633\n",
      "Epoch [52/500], Loss: 1.6977, Testloss: 1.8032\n",
      "Epoch [53/500], Loss: 1.6572, Testloss: 1.7356\n",
      "Epoch [54/500], Loss: 1.5989, Testloss: 1.6911\n",
      "Epoch [55/500], Loss: 1.5431, Testloss: 1.6225\n",
      "Epoch [56/500], Loss: 1.4888, Testloss: 1.5600\n",
      "Epoch [57/500], Loss: 1.4459, Testloss: 1.5297\n",
      "Epoch [58/500], Loss: 1.3914, Testloss: 1.4776\n",
      "Epoch [59/500], Loss: 1.3512, Testloss: 1.4128\n",
      "Epoch [60/500], Loss: 1.2999, Testloss: 1.3676\n",
      "Epoch [61/500], Loss: 1.2541, Testloss: 1.3243\n",
      "Epoch [62/500], Loss: 1.2193, Testloss: 1.2908\n",
      "Epoch [63/500], Loss: 1.1704, Testloss: 1.2442\n",
      "Epoch [64/500], Loss: 1.1345, Testloss: 1.1996\n",
      "Epoch [65/500], Loss: 1.1082, Testloss: 1.1531\n",
      "Epoch [66/500], Loss: 1.0517, Testloss: 1.1075\n",
      "Epoch [67/500], Loss: 1.0181, Testloss: 1.0782\n",
      "Epoch [68/500], Loss: 0.9905, Testloss: 1.0460\n",
      "Epoch [69/500], Loss: 0.9474, Testloss: 1.0047\n",
      "Epoch [70/500], Loss: 0.9090, Testloss: 0.9728\n",
      "Epoch [71/500], Loss: 0.8774, Testloss: 0.9329\n",
      "Epoch [72/500], Loss: 0.8578, Testloss: 0.9047\n",
      "Epoch [73/500], Loss: 0.8262, Testloss: 0.8686\n",
      "Epoch [74/500], Loss: 0.7854, Testloss: 0.8334\n",
      "Epoch [75/500], Loss: 0.7616, Testloss: 0.8095\n",
      "Epoch [76/500], Loss: 0.7343, Testloss: 0.7893\n",
      "Epoch [77/500], Loss: 0.7082, Testloss: 0.7547\n",
      "Epoch [78/500], Loss: 0.6880, Testloss: 0.7311\n",
      "Epoch [79/500], Loss: 0.6561, Testloss: 0.7053\n",
      "Epoch [80/500], Loss: 0.6379, Testloss: 0.6745\n",
      "Epoch [81/500], Loss: 0.6165, Testloss: 0.6588\n",
      "Epoch [82/500], Loss: 0.5899, Testloss: 0.6364\n",
      "Epoch [83/500], Loss: 0.5669, Testloss: 0.6115\n",
      "Epoch [84/500], Loss: 0.5463, Testloss: 0.5905\n",
      "Epoch [85/500], Loss: 0.5317, Testloss: 0.5609\n",
      "Epoch [86/500], Loss: 0.5144, Testloss: 0.5503\n",
      "Epoch [87/500], Loss: 0.4971, Testloss: 0.5298\n",
      "Epoch [88/500], Loss: 0.4844, Testloss: 0.5116\n",
      "Epoch [89/500], Loss: 0.4697, Testloss: 0.4938\n",
      "Epoch [90/500], Loss: 0.4454, Testloss: 0.4746\n",
      "Epoch [91/500], Loss: 0.4289, Testloss: 0.4618\n",
      "Epoch [92/500], Loss: 0.4161, Testloss: 0.4471\n",
      "Epoch [93/500], Loss: 0.3943, Testloss: 0.4290\n",
      "Epoch [94/500], Loss: 0.3858, Testloss: 0.4155\n",
      "Epoch [95/500], Loss: 0.3767, Testloss: 0.4024\n",
      "Epoch [96/500], Loss: 0.3642, Testloss: 0.3915\n",
      "Epoch [97/500], Loss: 0.3465, Testloss: 0.3752\n",
      "Epoch [98/500], Loss: 0.3342, Testloss: 0.3645\n",
      "Epoch [99/500], Loss: 0.3283, Testloss: 0.3544\n",
      "Epoch [100/500], Loss: 0.3192, Testloss: 0.3448\n",
      "Epoch [101/500], Loss: 0.3073, Testloss: 0.3333\n",
      "Epoch [102/500], Loss: 0.3011, Testloss: 0.3230\n",
      "Epoch [103/500], Loss: 0.2887, Testloss: 0.3157\n",
      "Epoch [104/500], Loss: 0.2791, Testloss: 0.3061\n",
      "Epoch [105/500], Loss: 0.2742, Testloss: 0.3015\n",
      "Epoch [106/500], Loss: 0.2708, Testloss: 0.2925\n",
      "Epoch [107/500], Loss: 0.2550, Testloss: 0.2826\n",
      "Epoch [108/500], Loss: 0.2519, Testloss: 0.2752\n",
      "Epoch [109/500], Loss: 0.2419, Testloss: 0.2672\n",
      "Epoch [110/500], Loss: 0.2405, Testloss: 0.2608\n",
      "Epoch [111/500], Loss: 0.2358, Testloss: 0.2566\n",
      "Epoch [112/500], Loss: 0.2246, Testloss: 0.2501\n",
      "Epoch [113/500], Loss: 0.2207, Testloss: 0.2437\n",
      "Epoch [114/500], Loss: 0.2173, Testloss: 0.2395\n",
      "Epoch [115/500], Loss: 0.2185, Testloss: 0.2356\n",
      "Epoch [116/500], Loss: 0.2079, Testloss: 0.2306\n",
      "Epoch [117/500], Loss: 0.2036, Testloss: 0.2245\n",
      "Epoch [118/500], Loss: 0.2027, Testloss: 0.2197\n",
      "Epoch [119/500], Loss: 0.1955, Testloss: 0.2151\n",
      "Epoch [120/500], Loss: 0.1924, Testloss: 0.2145\n",
      "Epoch [121/500], Loss: 0.1907, Testloss: 0.2117\n",
      "Epoch [122/500], Loss: 0.1900, Testloss: 0.2084\n",
      "Epoch [123/500], Loss: 0.1858, Testloss: 0.2076\n",
      "Epoch [124/500], Loss: 0.1849, Testloss: 0.2040\n",
      "Epoch [125/500], Loss: 0.1778, Testloss: 0.1985\n",
      "Epoch [126/500], Loss: 0.1804, Testloss: 0.1967\n",
      "Epoch [127/500], Loss: 0.1757, Testloss: 0.1973\n",
      "Epoch [128/500], Loss: 0.1765, Testloss: 0.1948\n",
      "Epoch [129/500], Loss: 0.1700, Testloss: 0.1912\n",
      "Epoch [130/500], Loss: 0.1715, Testloss: 0.1903\n",
      "Epoch [131/500], Loss: 0.1698, Testloss: 0.1901\n",
      "Epoch [132/500], Loss: 0.1673, Testloss: 0.1881\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "epochs = 500  # Number of epochs to train\n",
    "criterion = nn.MSELoss(reduction='mean')  # Mean Squared Error Loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0 \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #model.train_enforce_constraints()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_dataloader):\n",
    "            model.eval()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "        avg_loss_test = running_loss / len(test_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Testloss: {avg_loss_test:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
