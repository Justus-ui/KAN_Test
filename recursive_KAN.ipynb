{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=True):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(input_size, self.hidden_size, num_layers = 1)\n",
    "        \n",
    "        # Fully connected layer to map the hidden state to output\n",
    "        self.fc = nn.Linear(self.hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(1)\n",
    "        x.size(2)\n",
    "        hidden_state = torch.ones(1,batch_size, self.hidden_size)#num_layer, batch_size, hidden_dim\n",
    "        iters = 10\n",
    "        print(x.shape)\n",
    "        out,h = self.gru(x, hidden_state)\n",
    "        print(out.shape)\n",
    "        print(self.fc(out[:, -1, :]).shape)\n",
    "        for t in range(iters):\n",
    "            out, hidden_state = self.gru(x, hidden_state)\n",
    "            x = self.fc(out[:, -1, :])\n",
    "            x = x.reshape(-1,1,input_size) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Lipschitz_GRU(nn.Module):\n",
    "    def __init__(self, h, activation = None):\n",
    "        super(Lipschitz_Linear, self).__init__()\n",
    "        self.B = 2 ## Upper bound on product of weights norms\n",
    "        self.lip_reg = 0.0005 ## learning rate on Lip regularisation !\n",
    "        self.order = float('inf') ### order of L_j norm\n",
    "        h = h\n",
    "        self.activation = activation\n",
    "        layers = []\n",
    "        self.linear_layers = []\n",
    "        self.Norm_constraints = torch.rand(len(h) - 1) * self.B\n",
    "        self.grads_norm_constraints = []\n",
    "        for layer in range(1,len(h)):\n",
    "            linear = nn.Linear(h[layer -1], h[layer])\n",
    "            layers.append(linear)\n",
    "            layers.append(nn.BatchNorm1d(h[layer], affine=True))\n",
    "            self.linear_layers.append(linear)\n",
    "            if activation is not None:\n",
    "                layers.append(self.activation())\n",
    "        self.univariate_nn = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.univariate_nn(x)\n",
    "    \n",
    "    def compute_constraint_gradient(self):\n",
    "        prod = torch.prod(self.Norm_constraints)\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            grad = torch.sum(self.linear_layers[i].weight.grad * (self.linear_layers[i].weight.data / (torch.linalg.matrix_norm(self.linear_layers[i].weight.data, ord = self.order)))) + self.lip_reg * (prod / self.Norm_constraints[i])  * torch.exp(7 * ((prod) - self.B))\n",
    "            self.grads_norm_constraints.append(grad)\n",
    "\n",
    "    def upper_lipschitz_bound(self):\n",
    "        return torch.prod(self.Norm_constraints)\n",
    "\n",
    "    def update_norm_constraints(self):\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            self.Norm_constraints[i] -= 0.001 * self.grads_norm_constraints[i]\n",
    "    \n",
    "    def project_on_norm_constraints(self):\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            self.linear_layers[i].weight.data *= self.Norm_constraints[i] / torch.linalg.matrix_norm(self.linear_layers[i].weight.data, ord = self.order)\n",
    "    \n",
    "    def train_enforce_constraints(self):\n",
    "        self.compute_constraint_gradient()\n",
    "        self.update_norm_constraints()\n",
    "        self.project_on_norm_constraints()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Lipschitz_Linear(nn.Module):\n",
    "    def __init__(self, h, activation = None):\n",
    "        super(Lipschitz_Linear, self).__init__()\n",
    "        self.B = 2 ## Upper bound on product of weights norms\n",
    "        self.lip_reg = 0.0005 ## learning rate on Lip regularisation !\n",
    "        self.order = float('inf') ### order of L_j norm\n",
    "        h = h\n",
    "        self.activation = activation\n",
    "        layers = []\n",
    "        self.linear_layers = []\n",
    "        self.Norm_constraints = torch.rand(len(h) - 1) * self.B\n",
    "        self.grads_norm_constraints = []\n",
    "        for layer in range(1,len(h)):\n",
    "            linear = nn.Linear(h[layer -1], h[layer])\n",
    "            layers.append(linear)\n",
    "            layers.append(nn.BatchNorm1d(h[layer], affine=True))\n",
    "            self.linear_layers.append(linear)\n",
    "            if activation is not None:\n",
    "                layers.append(self.activation())\n",
    "        self.univariate_nn = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.univariate_nn(x)\n",
    "    \n",
    "    def compute_constraint_gradient(self):\n",
    "        prod = torch.prod(self.Norm_constraints)\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            grad = torch.sum(self.linear_layers[i].weight.grad * (self.linear_layers[i].weight.data / (torch.linalg.matrix_norm(self.linear_layers[i].weight.data, ord = self.order)))) + self.lip_reg * (prod / self.Norm_constraints[i])  * torch.exp(7 * ((prod) - self.B))\n",
    "            self.grads_norm_constraints.append(grad)\n",
    "\n",
    "    def upper_lipschitz_bound(self):\n",
    "        return torch.prod(self.Norm_constraints)\n",
    "\n",
    "    def update_norm_constraints(self):\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            self.Norm_constraints[i] -= 0.001 * self.grads_norm_constraints[i]\n",
    "    \n",
    "    def project_on_norm_constraints(self):\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            self.linear_layers[i].weight.data *= self.Norm_constraints[i] / torch.linalg.matrix_norm(self.linear_layers[i].weight.data, ord = self.order)\n",
    "    \n",
    "    def train_enforce_constraints(self):\n",
    "        self.compute_constraint_gradient()\n",
    "        self.update_norm_constraints()\n",
    "        self.project_on_norm_constraints()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.3302\n",
      "Epoch [2/1000], Loss: 0.2347\n",
      "Epoch [3/1000], Loss: 0.2239\n",
      "Epoch [4/1000], Loss: 0.2032\n",
      "Epoch [5/1000], Loss: 0.1803\n",
      "Epoch [6/1000], Loss: 0.1580\n",
      "Epoch [7/1000], Loss: 0.1432\n",
      "Epoch [8/1000], Loss: 0.1309\n",
      "Epoch [9/1000], Loss: 0.1215\n",
      "Epoch [10/1000], Loss: 0.1121\n",
      "Epoch [11/1000], Loss: 0.1058\n",
      "Epoch [12/1000], Loss: 0.1012\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\recursive_KAN.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m outputs \u001b[39m=\u001b[39m test(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m test\u001b[39m.\u001b[39mtrain_enforce_constraints()\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "test = Lipschitz_Linear([1,4,16,4,1], activation = nn.ReLU)\n",
    "def f(X):\n",
    "    return X**0.23\n",
    "X = torch.vstack((torch.rand(1000, 1), torch.zeros(1000 //20 ,1)))\n",
    "train_dataset = torch.utils.data.TensorDataset(X, f(X))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "epochs = 1000  # Number of epochs to train\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.RAdam(test.parameters(), lr=0.001)\n",
    "for epoch in range(epochs):\n",
    "    test.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = test(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        test.train_enforce_constraints()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "print(\"Training Complete!\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3370)\n"
     ]
    }
   ],
   "source": [
    "test.upper_lipschitz_bound()\n",
    "x = 1\n",
    "for layers in test.linear_layers:\n",
    "    x *= torch.linalg.matrix_norm(layers.weight.data, ord = float('inf'))\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super(NN, self).__init__()\n",
    "        h = [1] + h\n",
    "        layers = []\n",
    "        for layer in range(1,len(h)):\n",
    "            layers.append(nn.Linear(h[layer -1], h[layer]))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.univariate_nn = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.univariate_nn(x)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAN_NN_Layer(nn.Module):\n",
    "    def __init__(self, in_dim, h, out_dim):\n",
    "        super(KAN_NN_Layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.Network_stack = []\n",
    "        for _ in range(self.in_dim):\n",
    "            #self.Network_stack.append(NN(h))\n",
    "            self.Network_stack.append(Lipschitz_Linear([1] + h, activation = nn.ReLU))\n",
    "\n",
    "        #self.fc2 = nn.Linear(in_dim * h[-1], out_dim)\n",
    "        self.fc2 = Lipschitz_Linear([in_dim * h[-1], out_dim])\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_list = []\n",
    "        for i in range(self.in_dim):\n",
    "            output_list.append(self.Network_stack[i](x[:,i].reshape(-1,1)))\n",
    "        #print(torch.cat(output_list, dim=1).shape)\n",
    "        return self.fc2(torch.cat(output_list, dim=1))\n",
    "\n",
    "    def train_enforce_constraints(self):\n",
    "        for Lip_lin in self.Network_stack:\n",
    "            Lip_lin.train_enforce_constraints()\n",
    "        self.fc2.train_enforce_constraints()\n",
    "\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAN_NN(nn.Module):\n",
    "    def __init__(self, h, hidden):\n",
    "        super(KAN_NN, self).__init__()\n",
    "        self.layers = []\n",
    "        for layer in range(1,len(h)):\n",
    "            self.layers.append(KAN_NN_Layer(h[layer -1],hidden,h[layer]))\n",
    "        self.Kan_nn = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.Kan_nn(x)\n",
    "    \n",
    "    def train_enforce_constraints(self):\n",
    "        for Lip_NN in self.layers:\n",
    "            Lip_NN.train_enforce_constraints()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 3\n",
    "model = KAN_NN([in_dim,4,2,1], [16,32,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of KAN_NN(\n",
       "  (Kan_nn): Sequential(\n",
       "    (0): KAN_NN_Layer(\n",
       "      (fc2): Lipschitz_Linear(\n",
       "        (univariate_nn): Sequential(\n",
       "          (0): Linear(in_features=48, out_features=4, bias=True)\n",
       "          (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): KAN_NN_Layer(\n",
       "      (fc2): Lipschitz_Linear(\n",
       "        (univariate_nn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "          (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): KAN_NN_Layer(\n",
       "      (fc2): Lipschitz_Linear(\n",
       "        (univariate_nn): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=1, bias=True)\n",
       "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # Fixed exponents between 0 and 1\n",
    "    alpha = 0.5\n",
    "    beta = 0.7\n",
    "    gamma = 0.3\n",
    "    delta = 0.8\n",
    "    epsilon = 0.6\n",
    "    omega = 0.4\n",
    "    \n",
    "    # First term: (x^alpha + (1 - x)^beta)^gamma\n",
    "    term1 = 10*torch.sum(x ** alpha, dim = 1)\n",
    "    term2 = 2*torch.sum((1.0001 - x) ** beta, dim = 1)\n",
    "    # Second term: (sin(2Ï€x)^delta)^epsilon\n",
    "    term3 = torch.abs(torch.sin(2 * torch.pi * torch.sum(x**delta, dim = 1)))\n",
    "    term4= torch.sum(x**0.3, dim = 1)\n",
    "    result = (torch.abs((torch.sin(2 * torch.pi * (term1 ** .5)) + torch.cos(20 * torch.pi * (term2 ** .4))))**omega + torch.abs((term3**.67 + term4**0.1)))\n",
    "    return torch.reshape(result, [result.shape[0], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(100, in_dim) \n",
    "train_dataset = torch.utils.data.TensorDataset(X, f(X))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "X = torch.rand(100, in_dim)\n",
    "test_dataset = torch.utils.data.TensorDataset(X, f(X))\n",
    "test_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 8.0042, Testloss: 9.7513\n",
      "Epoch [2/500], Loss: 7.7207, Testloss: 9.5853\n",
      "Epoch [3/500], Loss: 7.5371, Testloss: 9.0409\n",
      "Epoch [4/500], Loss: 7.3163, Testloss: 8.6574\n",
      "Epoch [5/500], Loss: 7.0815, Testloss: 8.3928\n",
      "Epoch [6/500], Loss: 6.8674, Testloss: 8.3848\n",
      "Epoch [7/500], Loss: 6.6953, Testloss: 8.0417\n",
      "Epoch [8/500], Loss: 6.4525, Testloss: 7.7709\n",
      "Epoch [9/500], Loss: 6.2585, Testloss: 7.6580\n",
      "Epoch [10/500], Loss: 6.0892, Testloss: 7.2810\n",
      "Epoch [11/500], Loss: 5.9286, Testloss: 7.1629\n",
      "Epoch [12/500], Loss: 5.7593, Testloss: 6.4877\n",
      "Epoch [13/500], Loss: 5.5732, Testloss: 6.4897\n",
      "Epoch [14/500], Loss: 5.4235, Testloss: 6.2900\n",
      "Epoch [15/500], Loss: 5.2606, Testloss: 6.1561\n",
      "Epoch [16/500], Loss: 5.1238, Testloss: 5.9762\n",
      "Epoch [17/500], Loss: 4.9837, Testloss: 5.7518\n",
      "Epoch [18/500], Loss: 4.8006, Testloss: 5.4967\n",
      "Epoch [19/500], Loss: 4.6763, Testloss: 5.3001\n",
      "Epoch [20/500], Loss: 4.5470, Testloss: 5.1228\n",
      "Epoch [21/500], Loss: 4.4223, Testloss: 4.9763\n",
      "Epoch [22/500], Loss: 4.2915, Testloss: 4.8139\n",
      "Epoch [23/500], Loss: 4.1587, Testloss: 4.6507\n",
      "Epoch [24/500], Loss: 4.0263, Testloss: 4.5082\n",
      "Epoch [25/500], Loss: 3.9597, Testloss: 4.3719\n",
      "Epoch [26/500], Loss: 3.8012, Testloss: 4.2279\n",
      "Epoch [27/500], Loss: 3.6789, Testloss: 4.1220\n",
      "Epoch [28/500], Loss: 3.5734, Testloss: 3.9541\n",
      "Epoch [29/500], Loss: 3.4663, Testloss: 3.8534\n",
      "Epoch [30/500], Loss: 3.3599, Testloss: 3.7150\n",
      "Epoch [31/500], Loss: 3.2806, Testloss: 3.6251\n",
      "Epoch [32/500], Loss: 3.1733, Testloss: 3.4870\n",
      "Epoch [33/500], Loss: 3.0976, Testloss: 3.4117\n",
      "Epoch [34/500], Loss: 3.0083, Testloss: 3.2968\n",
      "Epoch [35/500], Loss: 2.9124, Testloss: 3.1784\n",
      "Epoch [36/500], Loss: 2.8252, Testloss: 3.0732\n",
      "Epoch [37/500], Loss: 2.7247, Testloss: 2.9764\n",
      "Epoch [38/500], Loss: 2.6547, Testloss: 2.8944\n",
      "Epoch [39/500], Loss: 2.5619, Testloss: 2.8134\n",
      "Epoch [40/500], Loss: 2.4979, Testloss: 2.7251\n",
      "Epoch [41/500], Loss: 2.4090, Testloss: 2.6316\n",
      "Epoch [42/500], Loss: 2.3275, Testloss: 2.5540\n",
      "Epoch [43/500], Loss: 2.2667, Testloss: 2.4733\n",
      "Epoch [44/500], Loss: 2.1892, Testloss: 2.4010\n",
      "Epoch [45/500], Loss: 2.1330, Testloss: 2.3096\n",
      "Epoch [46/500], Loss: 2.0630, Testloss: 2.2373\n",
      "Epoch [47/500], Loss: 2.0017, Testloss: 2.1690\n",
      "Epoch [48/500], Loss: 1.9428, Testloss: 2.1039\n",
      "Epoch [49/500], Loss: 1.8717, Testloss: 2.0323\n",
      "Epoch [50/500], Loss: 1.8264, Testloss: 1.9679\n",
      "Epoch [51/500], Loss: 1.7518, Testloss: 1.8996\n",
      "Epoch [52/500], Loss: 1.7020, Testloss: 1.8410\n",
      "Epoch [53/500], Loss: 1.6527, Testloss: 1.7782\n",
      "Epoch [54/500], Loss: 1.5908, Testloss: 1.7281\n",
      "Epoch [55/500], Loss: 1.5433, Testloss: 1.6689\n",
      "Epoch [56/500], Loss: 1.4947, Testloss: 1.6110\n",
      "Epoch [57/500], Loss: 1.4429, Testloss: 1.5528\n",
      "Epoch [58/500], Loss: 1.3962, Testloss: 1.5049\n",
      "Epoch [59/500], Loss: 1.3461, Testloss: 1.4581\n",
      "Epoch [60/500], Loss: 1.3002, Testloss: 1.4011\n",
      "Epoch [61/500], Loss: 1.2553, Testloss: 1.3455\n",
      "Epoch [62/500], Loss: 1.2264, Testloss: 1.3119\n",
      "Epoch [63/500], Loss: 1.1728, Testloss: 1.2577\n",
      "Epoch [64/500], Loss: 1.1314, Testloss: 1.2205\n",
      "Epoch [65/500], Loss: 1.0871, Testloss: 1.1801\n",
      "Epoch [66/500], Loss: 1.0567, Testloss: 1.1392\n",
      "Epoch [67/500], Loss: 1.0166, Testloss: 1.1028\n",
      "Epoch [68/500], Loss: 0.9840, Testloss: 1.0630\n",
      "Epoch [69/500], Loss: 0.9599, Testloss: 1.0230\n",
      "Epoch [70/500], Loss: 0.9126, Testloss: 0.9904\n",
      "Epoch [71/500], Loss: 0.8890, Testloss: 0.9553\n",
      "Epoch [72/500], Loss: 0.8510, Testloss: 0.9209\n",
      "Epoch [73/500], Loss: 0.8296, Testloss: 0.8914\n",
      "Epoch [74/500], Loss: 0.7929, Testloss: 0.8619\n",
      "Epoch [75/500], Loss: 0.7603, Testloss: 0.8275\n",
      "Epoch [76/500], Loss: 0.7348, Testloss: 0.8013\n",
      "Epoch [77/500], Loss: 0.7188, Testloss: 0.7741\n",
      "Epoch [78/500], Loss: 0.6911, Testloss: 0.7456\n",
      "Epoch [79/500], Loss: 0.6638, Testloss: 0.7183\n",
      "Epoch [80/500], Loss: 0.6381, Testloss: 0.6962\n",
      "Epoch [81/500], Loss: 0.6116, Testloss: 0.6688\n",
      "Epoch [82/500], Loss: 0.5890, Testloss: 0.6453\n",
      "Epoch [83/500], Loss: 0.5726, Testloss: 0.6269\n",
      "Epoch [84/500], Loss: 0.5428, Testloss: 0.6079\n",
      "Epoch [85/500], Loss: 0.5354, Testloss: 0.5834\n",
      "Epoch [86/500], Loss: 0.5134, Testloss: 0.5635\n",
      "Epoch [87/500], Loss: 0.4928, Testloss: 0.5462\n",
      "Epoch [88/500], Loss: 0.4787, Testloss: 0.5242\n",
      "Epoch [89/500], Loss: 0.4683, Testloss: 0.5084\n",
      "Epoch [90/500], Loss: 0.4501, Testloss: 0.4904\n",
      "Epoch [91/500], Loss: 0.4278, Testloss: 0.4774\n",
      "Epoch [92/500], Loss: 0.4094, Testloss: 0.4577\n",
      "Epoch [93/500], Loss: 0.4019, Testloss: 0.4406\n",
      "Epoch [94/500], Loss: 0.3890, Testloss: 0.4243\n",
      "Epoch [95/500], Loss: 0.3802, Testloss: 0.4182\n",
      "Epoch [96/500], Loss: 0.3612, Testloss: 0.3977\n",
      "Epoch [97/500], Loss: 0.3498, Testloss: 0.3857\n",
      "Epoch [98/500], Loss: 0.3341, Testloss: 0.3771\n",
      "Epoch [99/500], Loss: 0.3300, Testloss: 0.3621\n",
      "Epoch [100/500], Loss: 0.3192, Testloss: 0.3535\n",
      "Epoch [101/500], Loss: 0.3065, Testloss: 0.3421\n",
      "Epoch [102/500], Loss: 0.2963, Testloss: 0.3339\n",
      "Epoch [103/500], Loss: 0.2816, Testloss: 0.3278\n",
      "Epoch [104/500], Loss: 0.2812, Testloss: 0.3178\n",
      "Epoch [105/500], Loss: 0.2720, Testloss: 0.3058\n",
      "Epoch [106/500], Loss: 0.2588, Testloss: 0.3011\n",
      "Epoch [107/500], Loss: 0.2618, Testloss: 0.2977\n",
      "Epoch [108/500], Loss: 0.2484, Testloss: 0.2842\n",
      "Epoch [109/500], Loss: 0.2429, Testloss: 0.2796\n",
      "Epoch [110/500], Loss: 0.2339, Testloss: 0.2736\n",
      "Epoch [111/500], Loss: 0.2322, Testloss: 0.2658\n",
      "Epoch [112/500], Loss: 0.2246, Testloss: 0.2628\n",
      "Epoch [113/500], Loss: 0.2235, Testloss: 0.2561\n",
      "Epoch [114/500], Loss: 0.2155, Testloss: 0.2478\n",
      "Epoch [115/500], Loss: 0.2103, Testloss: 0.2453\n",
      "Epoch [116/500], Loss: 0.2070, Testloss: 0.2380\n",
      "Epoch [117/500], Loss: 0.2060, Testloss: 0.2371\n",
      "Epoch [118/500], Loss: 0.2007, Testloss: 0.2315\n",
      "Epoch [119/500], Loss: 0.1964, Testloss: 0.2271\n",
      "Epoch [120/500], Loss: 0.1929, Testloss: 0.2245\n",
      "Epoch [121/500], Loss: 0.1958, Testloss: 0.2149\n",
      "Epoch [122/500], Loss: 0.1860, Testloss: 0.2195\n",
      "Epoch [123/500], Loss: 0.1870, Testloss: 0.2103\n",
      "Epoch [124/500], Loss: 0.1833, Testloss: 0.2071\n",
      "Epoch [125/500], Loss: 0.1788, Testloss: 0.2043\n",
      "Epoch [126/500], Loss: 0.1784, Testloss: 0.2027\n",
      "Epoch [127/500], Loss: 0.1778, Testloss: 0.2037\n",
      "Epoch [128/500], Loss: 0.1728, Testloss: 0.1981\n",
      "Epoch [129/500], Loss: 0.1689, Testloss: 0.1955\n",
      "Epoch [130/500], Loss: 0.1676, Testloss: 0.1926\n",
      "Epoch [131/500], Loss: 0.1756, Testloss: 0.1939\n",
      "Epoch [132/500], Loss: 0.1702, Testloss: 0.1942\n",
      "Epoch [133/500], Loss: 0.1672, Testloss: 0.1907\n",
      "Epoch [134/500], Loss: 0.1644, Testloss: 0.1908\n",
      "Epoch [135/500], Loss: 0.1637, Testloss: 0.1899\n",
      "Epoch [136/500], Loss: 0.1610, Testloss: 0.1931\n",
      "Epoch [137/500], Loss: 0.1663, Testloss: 0.1900\n",
      "Epoch [138/500], Loss: 0.1600, Testloss: 0.1822\n",
      "Epoch [139/500], Loss: 0.1589, Testloss: 0.1843\n",
      "Epoch [140/500], Loss: 0.1596, Testloss: 0.1857\n",
      "Epoch [141/500], Loss: 0.1586, Testloss: 0.1816\n",
      "Epoch [142/500], Loss: 0.1621, Testloss: 0.1850\n",
      "Epoch [143/500], Loss: 0.1566, Testloss: 0.1852\n",
      "Epoch [144/500], Loss: 0.1570, Testloss: 0.1817\n",
      "Epoch [145/500], Loss: 0.1541, Testloss: 0.1796\n",
      "Epoch [146/500], Loss: 0.1554, Testloss: 0.1807\n",
      "Epoch [147/500], Loss: 0.1545, Testloss: 0.1787\n",
      "Epoch [148/500], Loss: 0.1538, Testloss: 0.1795\n",
      "Epoch [149/500], Loss: 0.1594, Testloss: 0.1770\n",
      "Epoch [150/500], Loss: 0.1529, Testloss: 0.1798\n",
      "Epoch [151/500], Loss: 0.1540, Testloss: 0.1779\n",
      "Epoch [152/500], Loss: 0.1530, Testloss: 0.1794\n",
      "Epoch [153/500], Loss: 0.1557, Testloss: 0.1784\n",
      "Epoch [154/500], Loss: 0.1512, Testloss: 0.1774\n",
      "Epoch [155/500], Loss: 0.1530, Testloss: 0.1762\n",
      "Epoch [156/500], Loss: 0.1546, Testloss: 0.1771\n",
      "Epoch [157/500], Loss: 0.1537, Testloss: 0.1754\n",
      "Epoch [158/500], Loss: 0.1514, Testloss: 0.1787\n",
      "Epoch [159/500], Loss: 0.1518, Testloss: 0.1767\n",
      "Epoch [160/500], Loss: 0.1520, Testloss: 0.1782\n",
      "Epoch [161/500], Loss: 0.1555, Testloss: 0.1773\n",
      "Epoch [162/500], Loss: 0.1529, Testloss: 0.1779\n",
      "Epoch [163/500], Loss: 0.1548, Testloss: 0.1793\n",
      "Epoch [164/500], Loss: 0.1540, Testloss: 0.1773\n",
      "Epoch [165/500], Loss: 0.1498, Testloss: 0.1761\n",
      "Epoch [166/500], Loss: 0.1517, Testloss: 0.1756\n",
      "Epoch [167/500], Loss: 0.1456, Testloss: 0.1785\n",
      "Epoch [168/500], Loss: 0.1486, Testloss: 0.1831\n",
      "Epoch [169/500], Loss: 0.1492, Testloss: 0.1813\n",
      "Epoch [170/500], Loss: 0.1514, Testloss: 0.1793\n",
      "Epoch [171/500], Loss: 0.1498, Testloss: 0.1808\n",
      "Epoch [172/500], Loss: 0.1504, Testloss: 0.1793\n",
      "Epoch [173/500], Loss: 0.1528, Testloss: 0.1804\n",
      "Epoch [174/500], Loss: 0.1538, Testloss: 0.1785\n",
      "Epoch [175/500], Loss: 0.1532, Testloss: 0.1768\n",
      "Epoch [176/500], Loss: 0.1550, Testloss: 0.1793\n",
      "Epoch [177/500], Loss: 0.1479, Testloss: 0.1788\n",
      "Epoch [178/500], Loss: 0.1515, Testloss: 0.1798\n",
      "Epoch [179/500], Loss: 0.1514, Testloss: 0.1807\n",
      "Epoch [180/500], Loss: 0.1549, Testloss: 0.1756\n",
      "Epoch [181/500], Loss: 0.1512, Testloss: 0.1760\n",
      "Epoch [182/500], Loss: 0.1541, Testloss: 0.1780\n",
      "Epoch [183/500], Loss: 0.1521, Testloss: 0.1767\n",
      "Epoch [184/500], Loss: 0.1555, Testloss: 0.1762\n",
      "Epoch [185/500], Loss: 0.1535, Testloss: 0.1758\n",
      "Epoch [186/500], Loss: 0.1490, Testloss: 0.1795\n",
      "Epoch [187/500], Loss: 0.1485, Testloss: 0.1812\n",
      "Epoch [188/500], Loss: 0.1483, Testloss: 0.1778\n",
      "Epoch [189/500], Loss: 0.1477, Testloss: 0.1765\n",
      "Epoch [190/500], Loss: 0.1520, Testloss: 0.1796\n",
      "Epoch [191/500], Loss: 0.1534, Testloss: 0.1761\n",
      "Epoch [192/500], Loss: 0.1524, Testloss: 0.1763\n",
      "Epoch [193/500], Loss: 0.1472, Testloss: 0.1780\n",
      "Epoch [194/500], Loss: 0.1530, Testloss: 0.1766\n",
      "Epoch [195/500], Loss: 0.1536, Testloss: 0.1771\n",
      "Epoch [196/500], Loss: 0.1491, Testloss: 0.1785\n",
      "Epoch [197/500], Loss: 0.1499, Testloss: 0.1769\n",
      "Epoch [198/500], Loss: 0.1516, Testloss: 0.1772\n",
      "Epoch [199/500], Loss: 0.1543, Testloss: 0.1768\n",
      "Epoch [200/500], Loss: 0.1557, Testloss: 0.1752\n",
      "Epoch [201/500], Loss: 0.1460, Testloss: 0.1749\n",
      "Epoch [202/500], Loss: 0.1520, Testloss: 0.1737\n",
      "Epoch [203/500], Loss: 0.1512, Testloss: 0.1762\n",
      "Epoch [204/500], Loss: 0.1499, Testloss: 0.1744\n",
      "Epoch [205/500], Loss: 0.1521, Testloss: 0.1778\n",
      "Epoch [206/500], Loss: 0.1497, Testloss: 0.1785\n",
      "Epoch [207/500], Loss: 0.1473, Testloss: 0.1779\n",
      "Epoch [208/500], Loss: 0.1487, Testloss: 0.1795\n",
      "Epoch [209/500], Loss: 0.1506, Testloss: 0.1772\n",
      "Epoch [210/500], Loss: 0.1545, Testloss: 0.1780\n",
      "Epoch [211/500], Loss: 0.1480, Testloss: 0.1794\n",
      "Epoch [212/500], Loss: 0.1522, Testloss: 0.1792\n",
      "Epoch [213/500], Loss: 0.1498, Testloss: 0.1803\n",
      "Epoch [214/500], Loss: 0.1518, Testloss: 0.1778\n",
      "Epoch [215/500], Loss: 0.1486, Testloss: 0.1767\n",
      "Epoch [216/500], Loss: 0.1541, Testloss: 0.1749\n",
      "Epoch [217/500], Loss: 0.1525, Testloss: 0.1776\n",
      "Epoch [218/500], Loss: 0.1527, Testloss: 0.1780\n",
      "Epoch [219/500], Loss: 0.1483, Testloss: 0.1791\n",
      "Epoch [220/500], Loss: 0.1474, Testloss: 0.1799\n",
      "Epoch [221/500], Loss: 0.1491, Testloss: 0.1800\n",
      "Epoch [222/500], Loss: 0.1497, Testloss: 0.1782\n",
      "Epoch [223/500], Loss: 0.1500, Testloss: 0.1778\n",
      "Epoch [224/500], Loss: 0.1467, Testloss: 0.1790\n",
      "Epoch [225/500], Loss: 0.1464, Testloss: 0.1777\n",
      "Epoch [226/500], Loss: 0.1521, Testloss: 0.1760\n",
      "Epoch [227/500], Loss: 0.1461, Testloss: 0.1775\n",
      "Epoch [228/500], Loss: 0.1518, Testloss: 0.1793\n",
      "Epoch [229/500], Loss: 0.1514, Testloss: 0.1794\n",
      "Epoch [230/500], Loss: 0.1523, Testloss: 0.1801\n",
      "Epoch [231/500], Loss: 0.1527, Testloss: 0.1806\n",
      "Epoch [232/500], Loss: 0.1480, Testloss: 0.1797\n",
      "Epoch [233/500], Loss: 0.1454, Testloss: 0.1769\n",
      "Epoch [234/500], Loss: 0.1468, Testloss: 0.1781\n",
      "Epoch [235/500], Loss: 0.1540, Testloss: 0.1800\n",
      "Epoch [236/500], Loss: 0.1534, Testloss: 0.1799\n",
      "Epoch [237/500], Loss: 0.1495, Testloss: 0.1798\n",
      "Epoch [238/500], Loss: 0.1500, Testloss: 0.1802\n",
      "Epoch [239/500], Loss: 0.1505, Testloss: 0.1792\n",
      "Epoch [240/500], Loss: 0.1503, Testloss: 0.1802\n",
      "Epoch [241/500], Loss: 0.1474, Testloss: 0.1785\n",
      "Epoch [242/500], Loss: 0.1475, Testloss: 0.1762\n",
      "Epoch [243/500], Loss: 0.1486, Testloss: 0.1786\n",
      "Epoch [244/500], Loss: 0.1528, Testloss: 0.1806\n",
      "Epoch [245/500], Loss: 0.1472, Testloss: 0.1784\n",
      "Epoch [246/500], Loss: 0.1517, Testloss: 0.1777\n",
      "Epoch [247/500], Loss: 0.1508, Testloss: 0.1761\n",
      "Epoch [248/500], Loss: 0.1508, Testloss: 0.1795\n",
      "Epoch [249/500], Loss: 0.1476, Testloss: 0.1786\n",
      "Epoch [250/500], Loss: 0.1520, Testloss: 0.1826\n",
      "Epoch [251/500], Loss: 0.1527, Testloss: 0.1833\n",
      "Epoch [252/500], Loss: 0.1485, Testloss: 0.1813\n",
      "Epoch [253/500], Loss: 0.1476, Testloss: 0.1781\n",
      "Epoch [254/500], Loss: 0.1504, Testloss: 0.1849\n",
      "Epoch [255/500], Loss: 0.1431, Testloss: 0.1803\n",
      "Epoch [256/500], Loss: 0.1513, Testloss: 0.1813\n",
      "Epoch [257/500], Loss: 0.1520, Testloss: 0.1820\n",
      "Epoch [258/500], Loss: 0.1459, Testloss: 0.1817\n",
      "Epoch [259/500], Loss: 0.1501, Testloss: 0.1806\n",
      "Epoch [260/500], Loss: 0.1447, Testloss: 0.1785\n",
      "Epoch [261/500], Loss: 0.1541, Testloss: 0.1782\n",
      "Epoch [262/500], Loss: 0.1484, Testloss: 0.1816\n",
      "Epoch [263/500], Loss: 0.1519, Testloss: 0.1787\n",
      "Epoch [264/500], Loss: 0.1485, Testloss: 0.1814\n",
      "Epoch [265/500], Loss: 0.1507, Testloss: 0.1796\n",
      "Epoch [266/500], Loss: 0.1496, Testloss: 0.1787\n",
      "Epoch [267/500], Loss: 0.1487, Testloss: 0.1808\n",
      "Epoch [268/500], Loss: 0.1444, Testloss: 0.1848\n",
      "Epoch [269/500], Loss: 0.1475, Testloss: 0.1874\n",
      "Epoch [270/500], Loss: 0.1553, Testloss: 0.1785\n",
      "Epoch [271/500], Loss: 0.1462, Testloss: 0.1810\n",
      "Epoch [272/500], Loss: 0.1451, Testloss: 0.1816\n",
      "Epoch [273/500], Loss: 0.1481, Testloss: 0.1831\n",
      "Epoch [274/500], Loss: 0.1533, Testloss: 0.1830\n",
      "Epoch [275/500], Loss: 0.1504, Testloss: 0.1813\n",
      "Epoch [276/500], Loss: 0.1466, Testloss: 0.1773\n",
      "Epoch [277/500], Loss: 0.1422, Testloss: 0.1807\n",
      "Epoch [278/500], Loss: 0.1427, Testloss: 0.1863\n",
      "Epoch [279/500], Loss: 0.1483, Testloss: 0.1829\n",
      "Epoch [280/500], Loss: 0.1573, Testloss: 0.1848\n",
      "Epoch [281/500], Loss: 0.1434, Testloss: 0.1821\n",
      "Epoch [282/500], Loss: 0.1405, Testloss: 0.1865\n",
      "Epoch [283/500], Loss: 0.1441, Testloss: 0.1844\n",
      "Epoch [284/500], Loss: 0.1416, Testloss: 0.1817\n",
      "Epoch [285/500], Loss: 0.1572, Testloss: 0.1816\n",
      "Epoch [286/500], Loss: 0.1465, Testloss: 0.1813\n",
      "Epoch [287/500], Loss: 0.1480, Testloss: 0.1823\n",
      "Epoch [288/500], Loss: 0.1484, Testloss: 0.1825\n",
      "Epoch [289/500], Loss: 0.1440, Testloss: 0.1800\n",
      "Epoch [290/500], Loss: 0.1431, Testloss: 0.1846\n",
      "Epoch [291/500], Loss: 0.1496, Testloss: 0.1815\n",
      "Epoch [292/500], Loss: 0.1432, Testloss: 0.1829\n",
      "Epoch [293/500], Loss: 0.1488, Testloss: 0.1830\n",
      "Epoch [294/500], Loss: 0.1446, Testloss: 0.1832\n",
      "Epoch [295/500], Loss: 0.1500, Testloss: 0.1806\n",
      "Epoch [296/500], Loss: 0.1468, Testloss: 0.1834\n",
      "Epoch [297/500], Loss: 0.1475, Testloss: 0.1905\n",
      "Epoch [298/500], Loss: 0.1433, Testloss: 0.1855\n",
      "Epoch [299/500], Loss: 0.1450, Testloss: 0.1855\n",
      "Epoch [300/500], Loss: 0.1514, Testloss: 0.1815\n",
      "Epoch [301/500], Loss: 0.1457, Testloss: 0.1838\n",
      "Epoch [302/500], Loss: 0.1459, Testloss: 0.1832\n",
      "Epoch [303/500], Loss: 0.1481, Testloss: 0.1869\n",
      "Epoch [304/500], Loss: 0.1432, Testloss: 0.1842\n",
      "Epoch [305/500], Loss: 0.1449, Testloss: 0.1800\n",
      "Epoch [306/500], Loss: 0.1495, Testloss: 0.1795\n",
      "Epoch [307/500], Loss: 0.1498, Testloss: 0.1782\n",
      "Epoch [308/500], Loss: 0.1496, Testloss: 0.1816\n",
      "Epoch [309/500], Loss: 0.1486, Testloss: 0.1793\n",
      "Epoch [310/500], Loss: 0.1414, Testloss: 0.1824\n",
      "Epoch [311/500], Loss: 0.1494, Testloss: 0.1837\n",
      "Epoch [312/500], Loss: 0.1500, Testloss: 0.1795\n",
      "Epoch [313/500], Loss: 0.1459, Testloss: 0.1838\n",
      "Epoch [314/500], Loss: 0.1452, Testloss: 0.1817\n",
      "Epoch [315/500], Loss: 0.1432, Testloss: 0.1805\n",
      "Epoch [316/500], Loss: 0.1495, Testloss: 0.1840\n",
      "Epoch [317/500], Loss: 0.1465, Testloss: 0.1840\n",
      "Epoch [318/500], Loss: 0.1457, Testloss: 0.1828\n",
      "Epoch [319/500], Loss: 0.1496, Testloss: 0.1829\n",
      "Epoch [320/500], Loss: 0.1405, Testloss: 0.1854\n",
      "Epoch [321/500], Loss: 0.1524, Testloss: 0.1847\n",
      "Epoch [322/500], Loss: 0.1434, Testloss: 0.1838\n",
      "Epoch [323/500], Loss: 0.1472, Testloss: 0.1832\n",
      "Epoch [324/500], Loss: 0.1441, Testloss: 0.1788\n",
      "Epoch [325/500], Loss: 0.1507, Testloss: 0.1867\n",
      "Epoch [326/500], Loss: 0.1444, Testloss: 0.1827\n",
      "Epoch [327/500], Loss: 0.1479, Testloss: 0.1796\n",
      "Epoch [328/500], Loss: 0.1490, Testloss: 0.1817\n",
      "Epoch [329/500], Loss: 0.1452, Testloss: 0.1794\n",
      "Epoch [330/500], Loss: 0.1453, Testloss: 0.1842\n",
      "Epoch [331/500], Loss: 0.1532, Testloss: 0.1807\n",
      "Epoch [332/500], Loss: 0.1409, Testloss: 0.1842\n",
      "Epoch [333/500], Loss: 0.1514, Testloss: 0.1831\n",
      "Epoch [334/500], Loss: 0.1506, Testloss: 0.1811\n",
      "Epoch [335/500], Loss: 0.1479, Testloss: 0.1822\n",
      "Epoch [336/500], Loss: 0.1440, Testloss: 0.1762\n",
      "Epoch [337/500], Loss: 0.1498, Testloss: 0.1799\n",
      "Epoch [338/500], Loss: 0.1471, Testloss: 0.1801\n",
      "Epoch [339/500], Loss: 0.1489, Testloss: 0.1838\n",
      "Epoch [340/500], Loss: 0.1505, Testloss: 0.1803\n",
      "Epoch [341/500], Loss: 0.1451, Testloss: 0.1825\n",
      "Epoch [342/500], Loss: 0.1475, Testloss: 0.1833\n",
      "Epoch [343/500], Loss: 0.1564, Testloss: 0.1823\n",
      "Epoch [344/500], Loss: 0.1449, Testloss: 0.1829\n",
      "Epoch [345/500], Loss: 0.1511, Testloss: 0.1829\n",
      "Epoch [346/500], Loss: 0.1453, Testloss: 0.1819\n",
      "Epoch [347/500], Loss: 0.1493, Testloss: 0.1813\n",
      "Epoch [348/500], Loss: 0.1498, Testloss: 0.1830\n",
      "Epoch [349/500], Loss: 0.1456, Testloss: 0.1783\n",
      "Epoch [350/500], Loss: 0.1445, Testloss: 0.1803\n",
      "Epoch [351/500], Loss: 0.1460, Testloss: 0.1813\n",
      "Epoch [352/500], Loss: 0.1504, Testloss: 0.1811\n",
      "Epoch [353/500], Loss: 0.1456, Testloss: 0.1811\n",
      "Epoch [354/500], Loss: 0.1442, Testloss: 0.1844\n",
      "Epoch [355/500], Loss: 0.1505, Testloss: 0.1814\n",
      "Epoch [356/500], Loss: 0.1501, Testloss: 0.1776\n",
      "Epoch [357/500], Loss: 0.1501, Testloss: 0.1792\n",
      "Epoch [358/500], Loss: 0.1460, Testloss: 0.1800\n",
      "Epoch [359/500], Loss: 0.1471, Testloss: 0.1794\n",
      "Epoch [360/500], Loss: 0.1488, Testloss: 0.1834\n",
      "Epoch [361/500], Loss: 0.1511, Testloss: 0.1780\n",
      "Epoch [362/500], Loss: 0.1447, Testloss: 0.1803\n",
      "Epoch [363/500], Loss: 0.1450, Testloss: 0.1828\n",
      "Epoch [364/500], Loss: 0.1459, Testloss: 0.1798\n",
      "Epoch [365/500], Loss: 0.1463, Testloss: 0.1796\n",
      "Epoch [366/500], Loss: 0.1443, Testloss: 0.1800\n",
      "Epoch [367/500], Loss: 0.1451, Testloss: 0.1838\n",
      "Epoch [368/500], Loss: 0.1454, Testloss: 0.1850\n",
      "Epoch [369/500], Loss: 0.1490, Testloss: 0.1809\n",
      "Epoch [370/500], Loss: 0.1502, Testloss: 0.1814\n",
      "Epoch [371/500], Loss: 0.1482, Testloss: 0.1837\n",
      "Epoch [372/500], Loss: 0.1435, Testloss: 0.1829\n",
      "Epoch [373/500], Loss: 0.1484, Testloss: 0.1832\n",
      "Epoch [374/500], Loss: 0.1494, Testloss: 0.1830\n",
      "Epoch [375/500], Loss: 0.1447, Testloss: 0.1772\n",
      "Epoch [376/500], Loss: 0.1497, Testloss: 0.1813\n",
      "Epoch [377/500], Loss: 0.1580, Testloss: 0.1808\n",
      "Epoch [378/500], Loss: 0.1495, Testloss: 0.1792\n",
      "Epoch [379/500], Loss: 0.1507, Testloss: 0.1793\n",
      "Epoch [380/500], Loss: 0.1458, Testloss: 0.1772\n",
      "Epoch [381/500], Loss: 0.1440, Testloss: 0.1812\n",
      "Epoch [382/500], Loss: 0.1411, Testloss: 0.1839\n",
      "Epoch [383/500], Loss: 0.1504, Testloss: 0.1837\n",
      "Epoch [384/500], Loss: 0.1499, Testloss: 0.1830\n",
      "Epoch [385/500], Loss: 0.1489, Testloss: 0.1847\n",
      "Epoch [386/500], Loss: 0.1440, Testloss: 0.1820\n",
      "Epoch [387/500], Loss: 0.1448, Testloss: 0.1822\n",
      "Epoch [388/500], Loss: 0.1498, Testloss: 0.1793\n",
      "Epoch [389/500], Loss: 0.1472, Testloss: 0.1855\n",
      "Epoch [390/500], Loss: 0.1473, Testloss: 0.1859\n",
      "Epoch [391/500], Loss: 0.1411, Testloss: 0.1837\n",
      "Epoch [392/500], Loss: 0.1487, Testloss: 0.1855\n",
      "Epoch [393/500], Loss: 0.1447, Testloss: 0.1817\n",
      "Epoch [394/500], Loss: 0.1501, Testloss: 0.1844\n",
      "Epoch [395/500], Loss: 0.1470, Testloss: 0.1807\n",
      "Epoch [396/500], Loss: 0.1483, Testloss: 0.1840\n",
      "Epoch [397/500], Loss: 0.1480, Testloss: 0.1778\n",
      "Epoch [398/500], Loss: 0.1440, Testloss: 0.1808\n",
      "Epoch [399/500], Loss: 0.1484, Testloss: 0.1795\n",
      "Epoch [400/500], Loss: 0.1496, Testloss: 0.1824\n",
      "Epoch [401/500], Loss: 0.1467, Testloss: 0.1836\n",
      "Epoch [402/500], Loss: 0.1462, Testloss: 0.1821\n",
      "Epoch [403/500], Loss: 0.1457, Testloss: 0.1839\n",
      "Epoch [404/500], Loss: 0.1447, Testloss: 0.1809\n",
      "Epoch [405/500], Loss: 0.1460, Testloss: 0.1813\n",
      "Epoch [406/500], Loss: 0.1483, Testloss: 0.1828\n",
      "Epoch [407/500], Loss: 0.1460, Testloss: 0.1814\n",
      "Epoch [408/500], Loss: 0.1447, Testloss: 0.1778\n",
      "Epoch [409/500], Loss: 0.1485, Testloss: 0.1817\n",
      "Epoch [410/500], Loss: 0.1480, Testloss: 0.1815\n",
      "Epoch [411/500], Loss: 0.1411, Testloss: 0.1823\n",
      "Epoch [412/500], Loss: 0.1442, Testloss: 0.1860\n",
      "Epoch [413/500], Loss: 0.1453, Testloss: 0.1790\n",
      "Epoch [414/500], Loss: 0.1463, Testloss: 0.1861\n",
      "Epoch [415/500], Loss: 0.1511, Testloss: 0.1803\n",
      "Epoch [416/500], Loss: 0.1518, Testloss: 0.1871\n",
      "Epoch [417/500], Loss: 0.1427, Testloss: 0.1842\n",
      "Epoch [418/500], Loss: 0.1504, Testloss: 0.1832\n",
      "Epoch [419/500], Loss: 0.1488, Testloss: 0.1830\n",
      "Epoch [420/500], Loss: 0.1461, Testloss: 0.1806\n",
      "Epoch [421/500], Loss: 0.1492, Testloss: 0.1818\n",
      "Epoch [422/500], Loss: 0.1449, Testloss: 0.1784\n",
      "Epoch [423/500], Loss: 0.1484, Testloss: 0.1802\n",
      "Epoch [424/500], Loss: 0.1465, Testloss: 0.1826\n",
      "Epoch [425/500], Loss: 0.1472, Testloss: 0.1815\n",
      "Epoch [426/500], Loss: 0.1465, Testloss: 0.1822\n",
      "Epoch [427/500], Loss: 0.1460, Testloss: 0.1849\n",
      "Epoch [428/500], Loss: 0.1517, Testloss: 0.1831\n",
      "Epoch [429/500], Loss: 0.1498, Testloss: 0.1795\n",
      "Epoch [430/500], Loss: 0.1434, Testloss: 0.1808\n",
      "Epoch [431/500], Loss: 0.1428, Testloss: 0.1853\n",
      "Epoch [432/500], Loss: 0.1470, Testloss: 0.1850\n",
      "Epoch [433/500], Loss: 0.1490, Testloss: 0.1802\n",
      "Epoch [434/500], Loss: 0.1539, Testloss: 0.1816\n",
      "Epoch [435/500], Loss: 0.1474, Testloss: 0.1801\n",
      "Epoch [436/500], Loss: 0.1445, Testloss: 0.1788\n",
      "Epoch [437/500], Loss: 0.1459, Testloss: 0.1811\n",
      "Epoch [438/500], Loss: 0.1485, Testloss: 0.1811\n",
      "Epoch [439/500], Loss: 0.1479, Testloss: 0.1804\n",
      "Epoch [440/500], Loss: 0.1468, Testloss: 0.1807\n",
      "Epoch [441/500], Loss: 0.1470, Testloss: 0.1815\n",
      "Epoch [442/500], Loss: 0.1489, Testloss: 0.1823\n",
      "Epoch [443/500], Loss: 0.1469, Testloss: 0.1808\n",
      "Epoch [444/500], Loss: 0.1415, Testloss: 0.1828\n",
      "Epoch [445/500], Loss: 0.1473, Testloss: 0.1844\n",
      "Epoch [446/500], Loss: 0.1548, Testloss: 0.1826\n",
      "Epoch [447/500], Loss: 0.1477, Testloss: 0.1843\n",
      "Epoch [448/500], Loss: 0.1454, Testloss: 0.1860\n",
      "Epoch [449/500], Loss: 0.1446, Testloss: 0.1820\n",
      "Epoch [450/500], Loss: 0.1454, Testloss: 0.1829\n",
      "Epoch [451/500], Loss: 0.1452, Testloss: 0.1839\n",
      "Epoch [452/500], Loss: 0.1452, Testloss: 0.1822\n",
      "Epoch [453/500], Loss: 0.1461, Testloss: 0.1813\n",
      "Epoch [454/500], Loss: 0.1495, Testloss: 0.1795\n",
      "Epoch [455/500], Loss: 0.1457, Testloss: 0.1796\n",
      "Epoch [456/500], Loss: 0.1458, Testloss: 0.1809\n",
      "Epoch [457/500], Loss: 0.1466, Testloss: 0.1829\n",
      "Epoch [458/500], Loss: 0.1434, Testloss: 0.1823\n",
      "Epoch [459/500], Loss: 0.1482, Testloss: 0.1837\n",
      "Epoch [460/500], Loss: 0.1452, Testloss: 0.1843\n",
      "Epoch [461/500], Loss: 0.1478, Testloss: 0.1797\n",
      "Epoch [462/500], Loss: 0.1495, Testloss: 0.1835\n",
      "Epoch [463/500], Loss: 0.1429, Testloss: 0.1858\n",
      "Epoch [464/500], Loss: 0.1477, Testloss: 0.1840\n",
      "Epoch [465/500], Loss: 0.1493, Testloss: 0.1830\n",
      "Epoch [466/500], Loss: 0.1459, Testloss: 0.1847\n",
      "Epoch [467/500], Loss: 0.1438, Testloss: 0.1827\n",
      "Epoch [468/500], Loss: 0.1497, Testloss: 0.1851\n",
      "Epoch [469/500], Loss: 0.1469, Testloss: 0.1867\n",
      "Epoch [470/500], Loss: 0.1400, Testloss: 0.1848\n",
      "Epoch [471/500], Loss: 0.1441, Testloss: 0.1857\n",
      "Epoch [472/500], Loss: 0.1486, Testloss: 0.1857\n",
      "Epoch [473/500], Loss: 0.1473, Testloss: 0.1813\n",
      "Epoch [474/500], Loss: 0.1435, Testloss: 0.1802\n",
      "Epoch [475/500], Loss: 0.1422, Testloss: 0.1856\n",
      "Epoch [476/500], Loss: 0.1520, Testloss: 0.1877\n",
      "Epoch [477/500], Loss: 0.1487, Testloss: 0.1860\n",
      "Epoch [478/500], Loss: 0.1415, Testloss: 0.1830\n",
      "Epoch [479/500], Loss: 0.1481, Testloss: 0.1864\n",
      "Epoch [480/500], Loss: 0.1434, Testloss: 0.1875\n",
      "Epoch [481/500], Loss: 0.1473, Testloss: 0.1857\n",
      "Epoch [482/500], Loss: 0.1457, Testloss: 0.1806\n",
      "Epoch [483/500], Loss: 0.1518, Testloss: 0.1893\n",
      "Epoch [484/500], Loss: 0.1454, Testloss: 0.1822\n",
      "Epoch [485/500], Loss: 0.1506, Testloss: 0.1848\n",
      "Epoch [486/500], Loss: 0.1449, Testloss: 0.1851\n",
      "Epoch [487/500], Loss: 0.1441, Testloss: 0.1833\n",
      "Epoch [488/500], Loss: 0.1423, Testloss: 0.1812\n",
      "Epoch [489/500], Loss: 0.1535, Testloss: 0.1842\n",
      "Epoch [490/500], Loss: 0.1473, Testloss: 0.1877\n",
      "Epoch [491/500], Loss: 0.1448, Testloss: 0.1844\n",
      "Epoch [492/500], Loss: 0.1478, Testloss: 0.1845\n",
      "Epoch [493/500], Loss: 0.1466, Testloss: 0.1821\n",
      "Epoch [494/500], Loss: 0.1466, Testloss: 0.1867\n",
      "Epoch [495/500], Loss: 0.1486, Testloss: 0.1858\n",
      "Epoch [496/500], Loss: 0.1400, Testloss: 0.1833\n",
      "Epoch [497/500], Loss: 0.1445, Testloss: 0.1838\n",
      "Epoch [498/500], Loss: 0.1448, Testloss: 0.1860\n",
      "Epoch [499/500], Loss: 0.1528, Testloss: 0.1863\n",
      "Epoch [500/500], Loss: 0.1442, Testloss: 0.1859\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Line2D.set() got an unexpected keyword argument 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\recursive_KAN.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     test_loss\u001b[39m.\u001b[39mappend(avg_loss_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Testloss: \u001b[39m\u001b[39m{\u001b[39;00mavg_loss_test\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m plt\u001b[39m.\u001b[39;49mplot(test_loss, title \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtest_loss\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m plt\u001b[39m.\u001b[39mlegend()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/recursive_KAN.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\pyplot.py:2740\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2738\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   2739\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2740\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[0;32m   2741\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[0;32m   2742\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1662\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1659\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> 1662\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1663\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   1664\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[0;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:544\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(result)\n\u001b[0;32m    543\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 544\u001b[0m     \u001b[39mreturn\u001b[39;00m [l[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m l \u001b[39min\u001b[39;49;00m result]\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:544\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(result)\n\u001b[0;32m    543\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 544\u001b[0m     \u001b[39mreturn\u001b[39;00m [l[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m l \u001b[39min\u001b[39;49;00m result]\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:537\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     labels \u001b[39m=\u001b[39m [label] \u001b[39m*\u001b[39m n_datasets\n\u001b[1;32m--> 537\u001b[0m result \u001b[39m=\u001b[39m (make_artist(x[:, j \u001b[39m%\u001b[39;49m ncx], y[:, j \u001b[39m%\u001b[39;49m ncy], kw,\n\u001b[0;32m    538\u001b[0m                       {\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, \u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m: label})\n\u001b[0;32m    539\u001b[0m           \u001b[39mfor\u001b[39;00m j, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(labels))\n\u001b[0;32m    541\u001b[0m \u001b[39mif\u001b[39;00m return_kwargs:\n\u001b[0;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:351\u001b[0m, in \u001b[0;36m_process_plot_var_args._makeline\u001b[1;34m(self, x, y, kw, kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m default_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getdefaults(\u001b[39mset\u001b[39m(), kw)\n\u001b[0;32m    350\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setdefaults(default_dict, kw)\n\u001b[1;32m--> 351\u001b[0m seg \u001b[39m=\u001b[39m mlines\u001b[39m.\u001b[39;49mLine2D(x, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    352\u001b[0m \u001b[39mreturn\u001b[39;00m seg, kw\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\_api\\deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[0;32m    449\u001b[0m     warn_deprecated(\n\u001b[0;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\lines.py:393\u001b[0m, in \u001b[0;36mLine2D.__init__\u001b[1;34m(self, xdata, ydata, linewidth, linestyle, color, gapcolor, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_markeredgewidth(markeredgewidth)\n\u001b[0;32m    391\u001b[0m \u001b[39m# update kwargs before updating data to give the caller a\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[39m# chance to init axes (and hence unit support)\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_update(kwargs)\n\u001b[0;32m    394\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pickradius \u001b[39m=\u001b[39m pickradius\n\u001b[0;32m    395\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mind_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:1186\u001b[0m, in \u001b[0;36mArtist._internal_update\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m   1179\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_internal_update\u001b[39m(\u001b[39mself\u001b[39m, kwargs):\n\u001b[0;32m   1180\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m \u001b[39m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m \u001b[39m    errors as if calling `set`.\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m \n\u001b[0;32m   1184\u001b[0m \u001b[39m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_props(\n\u001b[0;32m   1187\u001b[0m         kwargs, \u001b[39m\"\u001b[39;49m\u001b[39m{cls.__name__}\u001b[39;49;00m\u001b[39m.set() got an unexpected keyword argument \u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m   1188\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m{prop_name!r}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:1160\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[1;34m(self, props, errfmt)\u001b[0m\n\u001b[0;32m   1158\u001b[0m             func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mset_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1159\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(func):\n\u001b[1;32m-> 1160\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m   1161\u001b[0m                     errfmt\u001b[39m.\u001b[39mformat(\u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m), prop_name\u001b[39m=\u001b[39mk))\n\u001b[0;32m   1162\u001b[0m             ret\u001b[39m.\u001b[39mappend(func(v))\n\u001b[0;32m   1163\u001b[0m \u001b[39mif\u001b[39;00m ret:\n",
      "\u001b[1;31mAttributeError\u001b[0m: Line2D.set() got an unexpected keyword argument 'title'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcw0lEQVR4nO3db2zdVf3A8U/b0VsItEzn2m0WKyiiAhturBYkiKk2gUz3wDjBbHPhj+AkuEZlY7CK6DoRyKIrLkwQH6ibEDDGLUOsLgapWdjWBGSDwMBNYwsT184iLWu/vweG+qvrYLf0z077eiX3wY7n3O+5Hkbf3H8tyLIsCwCABBSO9QYAAI6VcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSkXe4/OEPf4h58+bF9OnTo6CgIH75y1++5Zpt27bFRz7ykcjlcvG+970v7r///iFsFQCY6PIOl66urpg5c2Y0NTUd0/wXXnghLrvssrjkkkuitbU1vvrVr8ZVV10VjzzySN6bBQAmtoK380sWCwoK4uGHH4758+cfdc6NN94Ymzdvjqeeeqp/7POf/3wcPHgwtm7dOtRLAwAT0KSRvkBLS0vU1tYOGKurq4uvfvWrR13T3d0d3d3d/X/u6+uLV155Jd75zndGQUHBSG0VABhGWZbFoUOHYvr06VFYODxvqx3xcGlra4vy8vIBY+Xl5dHZ2Rn//ve/48QTTzxiTWNjY9x6660jvTUAYBTs378/3v3udw/LfY14uAzFihUror6+vv/PHR0dcdppp8X+/fujtLR0DHcGAByrzs7OqKysjFNOOWXY7nPEw6WioiLa29sHjLW3t0dpaemgz7ZERORyucjlckeMl5aWChcASMxwvs1jxL/HpaamJpqbmweMPfroo1FTUzPSlwYAxpm8w+Vf//pXtLa2Rmtra0T85+POra2tsW/fvoj4z8s8ixYt6p9/7bXXxt69e+Mb3/hG7NmzJ+6+++74xS9+EcuWLRueRwAATBh5h8sTTzwR5513Xpx33nkREVFfXx/nnXderFq1KiIi/v73v/dHTETEe9/73ti8eXM8+uijMXPmzLjzzjvjRz/6UdTV1Q3TQwAAJoq39T0uo6WzszPKysqio6PDe1wAIBEj8fPb7yoCAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZQwqXpqamqKqqipKSkqiuro7t27e/6fy1a9fGBz7wgTjxxBOjsrIyli1bFq+99tqQNgwATFx5h8umTZuivr4+GhoaYufOnTFz5syoq6uLl156adD5P/vZz2L58uXR0NAQu3fvjnvvvTc2bdoUN91009vePAAwseQdLnfddVdcffXVsWTJkvjQhz4U69evj5NOOinuu+++Qec//vjjceGFF8YVV1wRVVVV8alPfSouv/zyt3yWBgDgf+UVLj09PbFjx46ora397x0UFkZtbW20tLQMuuaCCy6IHTt29IfK3r17Y8uWLXHppZce9Trd3d3R2dk54AYAMCmfyQcOHIje3t4oLy8fMF5eXh579uwZdM0VV1wRBw4ciI997GORZVkcPnw4rr322jd9qaixsTFuvfXWfLYGAEwAI/6pom3btsXq1avj7rvvjp07d8ZDDz0Umzdvjttuu+2oa1asWBEdHR39t/3794/0NgGABOT1jMuUKVOiqKgo2tvbB4y3t7dHRUXFoGtuueWWWLhwYVx11VUREXHOOedEV1dXXHPNNbFy5cooLDyynXK5XORyuXy2BgBMAHk941JcXByzZ8+O5ubm/rG+vr5obm6OmpqaQde8+uqrR8RJUVFRRERkWZbvfgGACSyvZ1wiIurr62Px4sUxZ86cmDt3bqxduza6urpiyZIlERGxaNGimDFjRjQ2NkZExLx58+Kuu+6K8847L6qrq+O5556LW265JebNm9cfMAAAxyLvcFmwYEG8/PLLsWrVqmhra4tZs2bF1q1b+9+wu2/fvgHPsNx8881RUFAQN998c/ztb3+Ld73rXTFv3rz4zne+M3yPAgCYEAqyBF6v6ezsjLKysujo6IjS0tKx3g4AcAxG4ue331UEACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhhQuTU1NUVVVFSUlJVFdXR3bt29/0/kHDx6MpUuXxrRp0yKXy8WZZ54ZW7ZsGdKGAYCJa1K+CzZt2hT19fWxfv36qK6ujrVr10ZdXV0888wzMXXq1CPm9/T0xCc/+cmYOnVqPPjggzFjxoz4y1/+Eqeeeupw7B8AmEAKsizL8llQXV0d559/fqxbty4iIvr6+qKysjKuv/76WL58+RHz169fH9/73vdiz549ccIJJwxpk52dnVFWVhYdHR1RWlo6pPsAAEbXSPz8zuulop6entixY0fU1tb+9w4KC6O2tjZaWloGXfOrX/0qampqYunSpVFeXh5nn312rF69Onp7e496ne7u7ujs7BxwAwDIK1wOHDgQvb29UV5ePmC8vLw82traBl2zd+/eePDBB6O3tze2bNkSt9xyS9x5553x7W9/+6jXaWxsjLKysv5bZWVlPtsEAMapEf9UUV9fX0ydOjXuueeemD17dixYsCBWrlwZ69evP+qaFStWREdHR/9t//79I71NACABeb05d8qUKVFUVBTt7e0Dxtvb26OiomLQNdOmTYsTTjghioqK+sc++MEPRltbW/T09ERxcfERa3K5XORyuXy2BgBMAHk941JcXByzZ8+O5ubm/rG+vr5obm6OmpqaQddceOGF8dxzz0VfX1//2LPPPhvTpk0bNFoAAI4m75eK6uvrY8OGDfGTn/wkdu/eHdddd110dXXFkiVLIiJi0aJFsWLFiv751113Xbzyyitxww03xLPPPhubN2+O1atXx9KlS4fvUQAAE0Le3+OyYMGCePnll2PVqlXR1tYWs2bNiq1bt/a/YXffvn1RWPjfHqqsrIxHHnkkli1bFueee27MmDEjbrjhhrjxxhuH71EAABNC3t/jMhZ8jwsApGfMv8cFAGAsCRcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIxpDCpampKaqqqqKkpCSqq6tj+/btx7Ru48aNUVBQEPPnzx/KZQGACS7vcNm0aVPU19dHQ0ND7Ny5M2bOnBl1dXXx0ksvvem6F198Mb72ta/FRRddNOTNAgATW97hctddd8XVV18dS5YsiQ996EOxfv36OOmkk+K+++476pre3t74whe+ELfeemucfvrpb3mN7u7u6OzsHHADAMgrXHp6emLHjh1RW1v73zsoLIza2tpoaWk56rpvfetbMXXq1LjyyiuP6TqNjY1RVlbWf6usrMxnmwDAOJVXuBw4cCB6e3ujvLx8wHh5eXm0tbUNuuaxxx6Le++9NzZs2HDM11mxYkV0dHT03/bv35/PNgGAcWrSSN75oUOHYuHChbFhw4aYMmXKMa/L5XKRy+VGcGcAQIryCpcpU6ZEUVFRtLe3Dxhvb2+PioqKI+Y///zz8eKLL8a8efP6x/r6+v5z4UmT4plnnokzzjhjKPsGACagvF4qKi4ujtmzZ0dzc3P/WF9fXzQ3N0dNTc0R888666x48skno7W1tf/26U9/Oi655JJobW313hUAIC95v1RUX18fixcvjjlz5sTcuXNj7dq10dXVFUuWLImIiEWLFsWMGTOisbExSkpK4uyzzx6w/tRTT42IOGIcAOCt5B0uCxYsiJdffjlWrVoVbW1tMWvWrNi6dWv/G3b37dsXhYW+kBcAGH4FWZZlY72Jt9LZ2RllZWXR0dERpaWlY70dAOAYjMTPb0+NAADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQjCGFS1NTU1RVVUVJSUlUV1fH9u3bjzp3w4YNcdFFF8XkyZNj8uTJUVtb+6bzAQCOJu9w2bRpU9TX10dDQ0Ps3LkzZs6cGXV1dfHSSy8NOn/btm1x+eWXx+9///toaWmJysrK+NSnPhV/+9vf3vbmAYCJpSDLsiyfBdXV1XH++efHunXrIiKir68vKisr4/rrr4/ly5e/5fre3t6YPHlyrFu3LhYtWjTonO7u7uju7u7/c2dnZ1RWVkZHR0eUlpbms10AYIx0dnZGWVnZsP78zusZl56entixY0fU1tb+9w4KC6O2tjZaWlqO6T5effXVeP311+Md73jHUec0NjZGWVlZ/62ysjKfbQIA41Re4XLgwIHo7e2N8vLyAePl5eXR1tZ2TPdx4403xvTp0wfEz/9asWJFdHR09N/279+fzzYBgHFq0mhebM2aNbFx48bYtm1blJSUHHVeLpeLXC43ijsDAFKQV7hMmTIlioqKor29fcB4e3t7VFRUvOnaO+64I9asWRO//e1v49xzz81/pwDAhJfXS0XFxcUxe/bsaG5u7h/r6+uL5ubmqKmpOeq622+/PW677bbYunVrzJkzZ+i7BQAmtLxfKqqvr4/FixfHnDlzYu7cubF27dro6uqKJUuWRETEokWLYsaMGdHY2BgREd/97ndj1apV8bOf/Syqqqr63wtz8sknx8knnzyMDwUAGO/yDpcFCxbEyy+/HKtWrYq2traYNWtWbN26tf8Nu/v27YvCwv8+kfPDH/4wenp64rOf/eyA+2loaIhvfvObb2/3AMCEkvf3uIyFkfgcOAAwssb8e1wAAMaScAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkDClcmpqaoqqqKkpKSqK6ujq2b9/+pvMfeOCBOOuss6KkpCTOOeec2LJly5A2CwBMbHmHy6ZNm6K+vj4aGhpi586dMXPmzKirq4uXXnpp0PmPP/54XH755XHllVfGrl27Yv78+TF//vx46qmn3vbmAYCJpSDLsiyfBdXV1XH++efHunXrIiKir68vKisr4/rrr4/ly5cfMX/BggXR1dUVv/71r/vHPvrRj8asWbNi/fr1g16ju7s7uru7+//c0dERp512Wuzfvz9KS0vz2S4AMEY6OzujsrIyDh48GGVlZcNyn5PymdzT0xM7duyIFStW9I8VFhZGbW1ttLS0DLqmpaUl6uvrB4zV1dXFL3/5y6Nep7GxMW699dYjxisrK/PZLgBwHPjHP/4xNuFy4MCB6O3tjfLy8gHj5eXlsWfPnkHXtLW1DTq/ra3tqNdZsWLFgNg5ePBgvOc974l9+/YN2wNnaN6oZ89+jT1ncfxwFscX53H8eOMVk3e84x3Ddp95hctoyeVykcvljhgvKyvzD+FxorS01FkcJ5zF8cNZHF+cx/GjsHD4PsSc1z1NmTIlioqKor29fcB4e3t7VFRUDLqmoqIir/kAAEeTV7gUFxfH7Nmzo7m5uX+sr68vmpubo6amZtA1NTU1A+ZHRDz66KNHnQ8AcDR5v1RUX18fixcvjjlz5sTcuXNj7dq10dXVFUuWLImIiEWLFsWMGTOisbExIiJuuOGGuPjii+POO++Myy67LDZu3BhPPPFE3HPPPcd8zVwuFw0NDYO+fMTochbHD2dx/HAWxxfncfwYibPI++PQERHr1q2L733ve9HW1hazZs2K73//+1FdXR0RER//+Mejqqoq7r///v75DzzwQNx8883x4osvxvvf//64/fbb49JLLx22BwEATAxDChcAgLHgdxUBAMkQLgBAMoQLAJAM4QIAJOO4CZempqaoqqqKkpKSqK6uju3bt7/p/AceeCDOOuusKCkpiXPOOSe2bNkySjsd//I5iw0bNsRFF10UkydPjsmTJ0dtbe1bnh3HLt+/F2/YuHFjFBQUxPz580d2gxNIvmdx8ODBWLp0aUybNi1yuVyceeaZ/j01TPI9i7Vr18YHPvCBOPHEE6OysjKWLVsWr7322ijtdvz6wx/+EPPmzYvp06dHQUHBm/4Owjds27YtPvKRj0Qul4v3ve99Az6BfMyy48DGjRuz4uLi7L777sv+/Oc/Z1dffXV26qmnZu3t7YPO/+Mf/5gVFRVlt99+e/b0009nN998c3bCCSdkTz755CjvfPzJ9yyuuOKKrKmpKdu1a1e2e/fu7Itf/GJWVlaW/fWvfx3lnY8/+Z7FG1544YVsxowZ2UUXXZR95jOfGZ3NjnP5nkV3d3c2Z86c7NJLL80ee+yx7IUXXsi2bduWtba2jvLOx598z+KnP/1plsvlsp/+9KfZCy+8kD3yyCPZtGnTsmXLlo3yzsefLVu2ZCtXrsweeuihLCKyhx9++E3n7927NzvppJOy+vr67Omnn85+8IMfZEVFRdnWrVvzuu5xES5z587Nli5d2v/n3t7ebPr06VljY+Og8z/3uc9ll1122YCx6urq7Etf+tKI7nMiyPcs/tfhw4ezU045JfvJT34yUlucMIZyFocPH84uuOCC7Ec/+lG2ePFi4TJM8j2LH/7wh9npp5+e9fT0jNYWJ4x8z2Lp0qXZJz7xiQFj9fX12YUXXjii+5xojiVcvvGNb2Qf/vCHB4wtWLAgq6ury+taY/5SUU9PT+zYsSNqa2v7xwoLC6O2tjZaWloGXdPS0jJgfkREXV3dUedzbIZyFv/r1Vdfjddff31YfxPoRDTUs/jWt74VU6dOjSuvvHI0tjkhDOUsfvWrX0VNTU0sXbo0ysvL4+yzz47Vq1dHb2/vaG17XBrKWVxwwQWxY8eO/peT9u7dG1u2bPElqGNguH52j/lvhz5w4ED09vZGeXn5gPHy8vLYs2fPoGva2toGnd/W1jZi+5wIhnIW/+vGG2+M6dOnH/EPJ/kZylk89thjce+990Zra+so7HDiGMpZ7N27N373u9/FF77whdiyZUs899xz8eUvfzlef/31aGhoGI1tj0tDOYsrrrgiDhw4EB/72Mciy7I4fPhwXHvttXHTTTeNxpb5f472s7uzszP+/e9/x4knnnhM9zPmz7gwfqxZsyY2btwYDz/8cJSUlIz1diaUQ4cOxcKFC2PDhg0xZcqUsd7OhNfX1xdTp06Ne+65J2bPnh0LFiyIlStXxvr168d6axPOtm3bYvXq1XH33XfHzp0746GHHorNmzfHbbfdNtZbY4jG/BmXKVOmRFFRUbS3tw8Yb29vj4qKikHXVFRU5DWfYzOUs3jDHXfcEWvWrInf/va3ce65547kNieEfM/i+eefjxdffDHmzZvXP9bX1xcREZMmTYpnnnkmzjjjjJHd9Dg1lL8X06ZNixNOOCGKior6xz74wQ9GW1tb9PT0RHFx8YjuebwaylnccsstsXDhwrjqqqsiIuKcc86Jrq6uuOaaa2LlypVRWOi/30fL0X52l5aWHvOzLRHHwTMuxcXFMXv27Ghubu4f6+vri+bm5qipqRl0TU1NzYD5ERGPPvroUedzbIZyFhERt99+e9x2222xdevWmDNnzmhsddzL9yzOOuusePLJJ6O1tbX/9ulPfzouueSSaG1tjcrKytHc/rgylL8XF154YTz33HP98RgR8eyzz8a0adNEy9swlLN49dVXj4iTN4Iy86v6RtWw/ezO733DI2Pjxo1ZLpfL7r///uzpp5/OrrnmmuzUU0/N2trasizLsoULF2bLly/vn//HP/4xmzRpUnbHHXdku3fvzhoaGnwcepjkexZr1qzJiouLswcffDD7+9//3n87dOjQWD2EcSPfs/hfPlU0fPI9i3379mWnnHJK9pWvfCV75plnsl//+tfZ1KlTs29/+9tj9RDGjXzPoqGhITvllFOyn//859nevXuz3/zmN9kZZ5yRfe5znxurhzBuHDp0KNu1a1e2a9euLCKyu+66K9u1a1f2l7/8JcuyLFu+fHm2cOHC/vlvfBz661//erZ79+6sqakp3Y9DZ1mW/eAHP8hOO+20rLi4OJs7d272pz/9qf9/u/jii7PFixcPmP+LX/wiO/PMM7Pi4uLswx/+cLZ58+ZR3vH4lc9ZvOc978ki4ohbQ0PD6G98HMr378X/J1yGV75n8fjjj2fV1dVZLpfLTj/99Ow73/lOdvjw4VHe9fiUz1m8/vrr2Te/+c3sjDPOyEpKSrLKysrsy1/+cvbPf/5z9Dc+zvz+978f9N//b/z/v3jx4uziiy8+Ys2sWbOy4uLi7PTTT89+/OMf533dgizzXBkAkIYxf48LAMCxEi4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJCM/wM9kKRvAVrZIAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "epochs = 500  # Number of epochs to train\n",
    "criterion = nn.MSELoss(reduction='mean')  # Mean Squared Error Loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0 \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #model.train_enforce_constraints()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    train_loss.append(avg_loss)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_dataloader):\n",
    "            model.eval()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "        avg_loss_test = running_loss / len(test_dataloader)\n",
    "    test_loss.append(avg_loss_test)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Testloss: {avg_loss_test:.4f}\")\n",
    "plt.plot(test_loss, title = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(train_loss, titel = 'train_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
