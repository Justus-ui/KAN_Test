{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lipschitz_Linear(nn.Module):\n",
    "    def __init__(self, h, activation = None):\n",
    "        super(Lipschitz_Linear, self).__init__()\n",
    "        self.B = 2 ## Upper bound on product of weights norms\n",
    "        self.lip_reg = 0.0005 ## learning rate on Lip regularisation !\n",
    "        self.order = float('inf') ### order of L_j norm\n",
    "        h = h\n",
    "        self.activation = activation\n",
    "        layers = []\n",
    "        self.linear_layers = []\n",
    "        self.Norm_constraints = torch.rand(len(h) - 1) * self.B\n",
    "        for layer in range(1,len(h)):\n",
    "            linear = nn.Linear(h[layer -1], h[layer])\n",
    "            layers.append(linear)\n",
    "            layers.append(nn.BatchNorm1d(h[layer], affine=True))\n",
    "            self.linear_layers.append(linear)\n",
    "            if activation is not None:\n",
    "                layers.append(self.activation())\n",
    "        self.univariate_nn = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.univariate_nn(x)\n",
    "    \n",
    "    def compute_constraint_gradient(self):\n",
    "        self.grads_norm_constraints = []\n",
    "        prod = torch.prod(self.Norm_constraints)\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            grad = torch.sum(self.linear_layers[i].weight.grad * (self.linear_layers[i].weight.data / (torch.linalg.matrix_norm(self.linear_layers[i].weight.data, ord = self.order)))) + self.lip_reg * (prod / self.Norm_constraints[i])  * torch.exp(7 * ((prod) - self.B))\n",
    "            self.grads_norm_constraints.append(grad)\n",
    "\n",
    "    def upper_lipschitz_bound(self):\n",
    "        return torch.prod(self.Norm_constraints)\n",
    "\n",
    "    def update_norm_constraints(self):\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            self.Norm_constraints[i] -= 0.001 * self.grads_norm_constraints[i]\n",
    "    \n",
    "    def project_on_norm_constraints(self):\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            self.linear_layers[i].weight.data *= self.Norm_constraints[i] / torch.linalg.matrix_norm(self.linear_layers[i].weight.data, ord = self.order)\n",
    "    \n",
    "    def train_enforce_constraints(self):\n",
    "        self.compute_constraint_gradient()\n",
    "        self.update_norm_constraints()\n",
    "        self.project_on_norm_constraints()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lipschitz_GRU(nn.Module):\n",
    "    def __init__(self, in_dim ,hidden, depth = 2, activation = nn.ReLU):\n",
    "        super(Lipschitz_GRU, self).__init__()\n",
    "        self.hidden = hidden\n",
    "        self.depth = depth\n",
    "\n",
    "        self.B = 2 ## Upper bound on product of weights norms\n",
    "        self.lip_reg = 0.0005 ## learning rate on Lip regularisation !\n",
    "        self.order = float('inf') ### order of L_j norm\n",
    "        self.activation = activation\n",
    "        self.Norm_constraints = torch.rand(self.depth) * self.B\n",
    "        self.gru = nn.GRU(in_dim, self.hidden, num_layers = self.depth, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, self.hidden_state = self.gru(x, self.hidden_state)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden_state = torch.zeros(self.depth, batch_size, self.hidden)\n",
    "    \n",
    "    def compute_constraint_gradient(self):\n",
    "        self.grads_norm_constraints = []\n",
    "        prod = torch.prod(self.Norm_constraints)\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            grad = torch.sum(self.linear_layers[i].weight.grad * (self.linear_layers[i].weight.data / (torch.linalg.matrix_norm(self.linear_layers[i].weight.data, ord = self.order)))) + self.lip_reg * (prod / self.Norm_constraints[i])  * torch.exp(7 * ((prod) - self.B))\n",
    "            self.grads_norm_constraints.append(grad)\n",
    "\n",
    "    def upper_lipschitz_bound(self):\n",
    "        return torch.prod(self.Norm_constraints)\n",
    "\n",
    "    def update_norm_constraints(self):\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            self.Norm_constraints[i] -= 0.001 * self.grads_norm_constraints[i]\n",
    "    \n",
    "    def project_on_norm_constraints(self):\n",
    "        for i in range(len(self.Norm_constraints)):\n",
    "            self.linear_layers[i].weight.data *= self.Norm_constraints[i] / torch.linalg.matrix_norm(self.linear_layers[i].weight.data, ord = self.order)\n",
    "    \n",
    "    def train_enforce_constraints(self):\n",
    "        self.compute_constraint_gradient()\n",
    "        self.update_norm_constraints()\n",
    "        self.project_on_norm_constraints()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, in_dim ,hidden, depth = 2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden = hidden\n",
    "        self.depth = depth\n",
    "        self.gru = nn.GRU(in_dim, self.hidden, num_layers = self.depth, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, self.hidden_state = self.gru(x, self.hidden_state)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden_state = torch.zeros(self.depth, batch_size, self.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAN_RNN_Layer(nn.Module):\n",
    "    def __init__(self, N_Agents, in_dim, hidden, depth):\n",
    "        \"\"\" \n",
    "        in_dim:Dimension of Agent information, i.e cartesian coordinates R^2\n",
    "        \"\"\"\n",
    "        super(KAN_RNN_Layer, self).__init__()\n",
    "        self.N_Agents = N_Agents\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden = hidden \n",
    "        self.Network_stack = []\n",
    "        self.linear_Network_stack = []\n",
    "        self.activation = nn.ReLU()\n",
    "        self.num_forward_steps = 10\n",
    "        for _ in range(self.N_Agents):\n",
    "            Networks = []\n",
    "            for _ in range(self.N_Agents):\n",
    "                Networks.append(Lipschitz_GRU(in_dim = self.in_dim ,hidden = self.hidden, depth = depth)) ## Dimension of input x -> indim, depth number of stacked Gru Layers, hidden Numer of Neurons in the GRu Layers\n",
    "            self.Network_stack.append(Networks)\n",
    "            self.linear_Network_stack.append(Lipschitz_Linear([N_Agents * hidden, self.in_dim]))\n",
    "\n",
    "    def time_step(self, x):\n",
    "        outs = torch.zeros_like(x)\n",
    "        for i in range(self.N_Agents): ## out\n",
    "            output_list = []\n",
    "            for j in range(self.N_Agents): ### in\n",
    "                output_list.append(self.Network_stack[i][j](x[:,j,:].unsqueeze(1)))\n",
    "            out = self.linear_Network_stack[i](self.activation(torch.cat(output_list, dim=1).reshape(-1, self.N_Agents * self.hidden)))\n",
    "            outs[i] = out\n",
    "        return outs\n",
    "\n",
    "    def system_dynamics(self,u, x_prev):\n",
    "        lam = 0.5\n",
    "        return x_prev + lam * u\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Inital States [Batch_size, N_Agents, in_dim] \n",
    "\n",
    "        return \n",
    "        control_trajectory: controller output [Num_timesteps ,Batch_size, N_Agents, in_dim]\n",
    "        outs: State of Agents [Num_timesteps ,Batch_size, N_Agents, in_dim]  \n",
    "\n",
    "        \"\"\" \n",
    "        outs = torch.zeros(self.num_forward_steps, *x.shape)\n",
    "        control_trajectory = torch.zeros(self.num_forward_steps, *x.shape) ## Assume u of same shape as x!\n",
    "        for i in range(self.num_forward_steps):\n",
    "            outs[i] = x\n",
    "            u = self.time_step(x)\n",
    "            x = self.system_dynamics(u, x)\n",
    "            control_trajectory[i] = u\n",
    "        return outs, control_trajectory\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        for lists in self.Network_stack:\n",
    "            for gru in lists:\n",
    "                gru.init_hidden(batch_size)\n",
    "\n",
    "    def train_enforce_constraints(self):\n",
    "        Lip_lin.train_enforce_constraints()\n",
    "        self.fc2.train_enforce_constraints()\n",
    "\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import dblquad\n",
    "\n",
    "def evaluate_integral(k, L):\n",
    "    \"\"\"\n",
    "    Evaluate the integral for multiple k and L values.\n",
    "    \n",
    "    Parameters:\n",
    "    k: List of k values [k1, k2, ...]\n",
    "    L: List of L values [L1, L2, ...]\n",
    "\n",
    "    Returns:\n",
    "    Tensor of the square root of the evaluated integrals.\n",
    "    \"\"\"\n",
    "    def integrand(*args):\n",
    "        result = 1.0\n",
    "        for i in range(len(k)):\n",
    "            result *= np.cos(k[i] * args[i])**2\n",
    "        return result\n",
    "\n",
    "    # Define the integration limits for all dimensions\n",
    "    limits = [(0, L) for L in L]\n",
    "\n",
    "    # Compute the nested integral\n",
    "    integral, _ = nquad(integrand, limits)\n",
    "\n",
    "    return torch.tensor(np.sqrt(integral))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "class Ergodicity_Loss(nn.Module):\n",
    "    def __init__(self, N_Agents, n_timesteps):\n",
    "        super(Ergodicity_Loss, self).__init__()\n",
    "        self.N_Agents = N_Agents\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.L = [3,3]\n",
    "\n",
    "    def compute_normalization_constant(k):\n",
    "        \"\"\" \n",
    "        h_k\n",
    "        \"\"\"\n",
    "        return evaluate_integral(k, self.L)\n",
    "\n",
    "    def fourier_basis(x, k):\n",
    "        \"\"\" \n",
    "        x: State at time t_k (N_Agents, in_dim)\n",
    "        Assume L = 2\n",
    "        \"\"\"\n",
    "        result = torch.tensor(1.0)\n",
    "        for i in range(len(k)):\n",
    "            result *= torch.cos(k[i] * x[:, i])\n",
    "        result *= self.compute_normalization_constant(k)\n",
    "        return result\n",
    "        \n",
    "\n",
    "    def compute_fourier_coefficients_agents(x, k):\n",
    "        \"\"\"\n",
    "        x: State of Agents [Num_timesteps ,Batch_size, N_Agents, in_dim] \n",
    "        \"\"\"\n",
    "        # For now i just put as calculaated t 1s\n",
    "        transform = fourier_basis(x.view(-1, self.in_dim),k)\n",
    "        result = transform.view(self.n_timesteps, -1 , self.N_Agents)\n",
    "        c_k = result.sum(dim=0).sum(dim=1, keepdim=True)         \n",
    "        return c_k\n",
    "\n",
    "    def compute_fourier_coefficients_density(x):\n",
    "        \"\"\"\n",
    "        x: State of Agents [Num_timesteps ,Batch_size, N_Agents, in_dim] \n",
    "        \"\"\"\n",
    "        # For now i just put as calculaated t 1s\n",
    "        transform = fourier_basis(x.view(-1, self.in_dim),k)\n",
    "        result = transform.view(self.n_timesteps, -1 , self.N_Agents)\n",
    "        c_k = result.sum(dim=0).sum(dim=1, keepdim=True)         \n",
    "        return c_k\n",
    "\n",
    "    def forward(x):\n",
    "        \"\"\"\n",
    "        x: State of Agents [Num_timesteps ,Batch_size, N_Agents, in_dim] \n",
    "        \"\"\"\n",
    "        loss = torch.zeros(1)\n",
    "        k = list(range(n+1))\n",
    "        for sets in product(k, repeat = 4):\n",
    "            loss += (self.compute_fourier_coefficients(x,k) - self.compute_fourier_coefficients_density(k))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = KAN_RNN_Layer(N_Agents = 4, in_dim = 2, hidden = 3, depth = 2)\n",
    "test.init_hidden(batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,4,2)\n",
    "x,u = test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5733, -1.1735],\n",
      "         [-0.2110,  1.3537],\n",
      "         [-0.8912,  0.4967],\n",
      "         [ 1.6755, -0.6769]],\n",
      "\n",
      "        [[-1.0037, -0.9008],\n",
      "         [ 1.1723,  1.1508],\n",
      "         [ 0.8098,  0.8319],\n",
      "         [-0.9783, -1.0819]],\n",
      "\n",
      "        [[-0.7334, -0.4555],\n",
      "         [ 1.3093, -0.9915],\n",
      "         [ 0.6001,  1.6201],\n",
      "         [-1.1761, -0.1730]],\n",
      "\n",
      "        [[-0.2924,  0.0295],\n",
      "         [-1.2782,  0.0803],\n",
      "         [ 0.0702, -1.4653],\n",
      "         [ 1.5005,  1.3555]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.0798, -0.9756],\n",
      "         [-0.8564,  1.3121],\n",
      "         [-0.8203,  0.6041],\n",
      "         [ 1.5969, -0.9406]],\n",
      "\n",
      "        [[-0.9404, -0.9134],\n",
      "         [ 1.1987,  1.1471],\n",
      "         [ 0.7762,  0.8373],\n",
      "         [-1.0345, -1.0710]],\n",
      "\n",
      "        [[-0.5329,  1.6368],\n",
      "         [ 1.3360, -0.9537],\n",
      "         [ 0.4941, -0.0795],\n",
      "         [-1.2972, -0.6036]],\n",
      "\n",
      "        [[ 0.2098, -0.1779],\n",
      "         [-1.4176, -0.1371],\n",
      "         [-0.1664, -1.2360],\n",
      "         [ 1.3742,  1.5510]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.5581, -0.7990],\n",
      "         [-1.3616,  1.0792],\n",
      "         [-0.4545,  0.8843],\n",
      "         [ 1.2580, -1.1645]],\n",
      "\n",
      "        [[-0.8753, -0.8364],\n",
      "         [ 1.2783,  1.1520],\n",
      "         [ 0.6680,  0.8225],\n",
      "         [-1.0710, -1.1381]],\n",
      "\n",
      "        [[-0.4436,  1.6543],\n",
      "         [ 1.3943, -1.0113],\n",
      "         [ 0.3624, -0.4034],\n",
      "         [-1.3130, -0.2396]],\n",
      "\n",
      "        [[ 0.8801, -0.1797],\n",
      "         [-1.2803, -0.7795],\n",
      "         [-0.6632, -0.7196],\n",
      "         [ 1.0634,  1.6788]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 1.0894, -0.4336],\n",
      "         [-1.6112,  0.7987],\n",
      "         [ 0.1426,  1.0567],\n",
      "         [ 0.3792, -1.4217]],\n",
      "\n",
      "        [[-0.4277, -0.5604],\n",
      "         [ 1.4015,  1.1963],\n",
      "         [ 0.3407,  0.6947],\n",
      "         [-1.3145, -1.3305]],\n",
      "\n",
      "        [[-0.1830,  1.5324],\n",
      "         [ 1.3889, -1.1843],\n",
      "         [ 0.2054, -0.4672],\n",
      "         [-1.4113,  0.1191]],\n",
      "\n",
      "        [[ 1.0936, -0.0958],\n",
      "         [-1.1166, -1.2382],\n",
      "         [-0.8684, -0.2117],\n",
      "         [ 0.8914,  1.5457]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 1.1527, -0.2074],\n",
      "         [-1.4013,  0.6251],\n",
      "         [ 0.6882,  1.1056],\n",
      "         [-0.4396, -1.5233]],\n",
      "\n",
      "        [[-0.1699, -0.2392],\n",
      "         [ 1.5546,  1.2486],\n",
      "         [-0.1516,  0.4628],\n",
      "         [-1.2330, -1.4723]],\n",
      "\n",
      "        [[ 0.0667,  1.5970],\n",
      "         [ 1.2661, -1.1513],\n",
      "         [ 0.2003, -0.2951],\n",
      "         [-1.5331, -0.1506]],\n",
      "\n",
      "        [[ 1.1396, -0.0773],\n",
      "         [-1.1196, -1.4567],\n",
      "         [-0.8589,  0.1866],\n",
      "         [ 0.8389,  1.3475]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.9130, -0.1216],\n",
      "         [-1.1845,  0.5063],\n",
      "         [ 1.0570,  1.1562],\n",
      "         [-0.7856, -1.5409]],\n",
      "\n",
      "        [[-0.1104, -0.0196],\n",
      "         [ 1.6492,  1.2719],\n",
      "         [-0.5794,  0.2672],\n",
      "         [-0.9594, -1.5194]],\n",
      "\n",
      "        [[ 0.2611,  1.6540],\n",
      "         [ 1.1665, -1.0033],\n",
      "         [ 0.1667, -0.1932],\n",
      "         [-1.5943, -0.4574]],\n",
      "\n",
      "        [[ 1.1459, -0.0503],\n",
      "         [-1.1889, -1.5779],\n",
      "         [-0.7744,  0.5285],\n",
      "         [ 0.8175,  1.0997]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.7395, -0.0695],\n",
      "         [-1.0861,  0.4796],\n",
      "         [ 1.2191,  1.1486],\n",
      "         [-0.8724, -1.5587]],\n",
      "\n",
      "        [[-0.3754,  0.0476],\n",
      "         [ 1.7082,  1.2552],\n",
      "         [-0.5253,  0.2351],\n",
      "         [-0.8075, -1.5379]],\n",
      "\n",
      "        [[ 0.3803,  1.6780],\n",
      "         [ 1.1122, -0.7988],\n",
      "         [ 0.1207, -0.1650],\n",
      "         [-1.6132, -0.7141]],\n",
      "\n",
      "        [[ 1.1164, -0.0627],\n",
      "         [-1.2536, -1.6067],\n",
      "         [-0.6955,  0.7862],\n",
      "         [ 0.8327,  0.8832]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.4920, -0.0305],\n",
      "         [-0.9475,  0.4977],\n",
      "         [ 1.3961,  1.1128],\n",
      "         [-0.9406, -1.5800]],\n",
      "\n",
      "        [[-0.7068,  0.0192],\n",
      "         [ 1.7187,  1.2671],\n",
      "         [-0.4021,  0.2414],\n",
      "         [-0.6097, -1.5278]],\n",
      "\n",
      "        [[ 0.3791,  1.6630],\n",
      "         [ 1.1339, -0.6476],\n",
      "         [ 0.0876, -0.1262],\n",
      "         [-1.6006, -0.8892]],\n",
      "\n",
      "        [[ 1.0904, -0.2281],\n",
      "         [-1.2552, -1.5327],\n",
      "         [-0.6976,  1.0134],\n",
      "         [ 0.8624,  0.7473]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.1693, -0.0079],\n",
      "         [-0.7055,  0.5248],\n",
      "         [ 1.5537,  1.0784],\n",
      "         [-1.0176, -1.5952]],\n",
      "\n",
      "        [[-1.0061, -0.0588],\n",
      "         [ 1.6600,  1.2953],\n",
      "         [-0.2685,  0.2627],\n",
      "         [-0.3855, -1.4993]],\n",
      "\n",
      "        [[ 0.3682,  1.6441],\n",
      "         [ 1.1725, -0.5577],\n",
      "         [ 0.0364, -0.1030],\n",
      "         [-1.5771, -0.9833]],\n",
      "\n",
      "        [[ 1.0544, -0.3503],\n",
      "         [-1.2323, -1.4518],\n",
      "         [-0.7322,  1.1611],\n",
      "         [ 0.9101,  0.6409]]], grad_fn=<CopySlices>)\n",
      "tensor([[[-0.4251, -0.1809],\n",
      "         [-0.3180,  0.6179],\n",
      "         [ 1.6798,  1.0992],\n",
      "         [-0.9367, -1.5362]],\n",
      "\n",
      "        [[-1.1384, -0.1641],\n",
      "         [ 1.6060,  1.3244],\n",
      "         [-0.1842,  0.2986],\n",
      "         [-0.2834, -1.4589]],\n",
      "\n",
      "        [[ 0.4546,  1.5774],\n",
      "         [ 1.1473, -0.4542],\n",
      "         [-0.0286,  0.0144],\n",
      "         [-1.5733, -1.1376]],\n",
      "\n",
      "        [[ 1.0096, -0.3861],\n",
      "         [-1.1817, -1.4108],\n",
      "         [-0.7982,  1.2430],\n",
      "         [ 0.9703,  0.5539]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
