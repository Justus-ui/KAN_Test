{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Kan_NN import Neural_Kan\n",
    "from kan import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define the objective function\n",
    "def f(X):\n",
    "    X_1 = X**(0.5)\n",
    "    X_2 = torch.sum(X_1, dim=1, keepdim=True)\n",
    "    X_3 = X_2**(.4521)\n",
    "    X_4 = torch.sum(X_3, dim=1, keepdim=True)\n",
    "    return X_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1]) tensor(3.5435) tensor(1.4634)\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    # Fixed exponents between 0 and 1\n",
    "    alpha = 0.5\n",
    "    beta = 0.7\n",
    "    gamma = 0.3\n",
    "    delta = 0.8\n",
    "    epsilon = 0.6\n",
    "    omega = 0.4\n",
    "    \n",
    "    # First term: (x^alpha + (1 - x)^beta)^gamma\n",
    "    term1 = 10*torch.sum(x ** alpha, dim = 1)\n",
    "    term2 = 2*torch.sum((1.0001 - x) ** beta, dim = 1)\n",
    "    # Second term: (sin(2πx)^delta)^epsilon\n",
    "    term3 = torch.abs(torch.sin(2 * torch.pi * torch.sum(x**delta, dim = 1)))\n",
    "    term4= torch.sum(x**0.3, dim = 1)\n",
    "    result = (torch.abs((torch.sin(2 * torch.pi * (term1 ** .5)) + torch.cos(20 * torch.pi * (term2 ** .4))))**omega + torch.abs((term3**.67 + term4**0.1)))\n",
    "    return torch.reshape(result, [result.shape[0], 1])\n",
    "    #term3 = torch.abs(torch.cos(20 * torch.pi * x)) ** .345\n",
    "    #term3 = term3 ** .2345\n",
    "    #term3 = 0.5*torch.sum(term3, dim = 1)\n",
    "\n",
    "    #result = ((term1 + term2) ** omega + term3) ** .9\n",
    "    #return result\n",
    "# Example usage:\n",
    "n = 10  # length of input tensor\n",
    "x = torch.rand(1000,n, dtype=torch.float32)  # Example input tensor of size n\n",
    "\n",
    "\n",
    "result = f(x)\n",
    "print(result.shape, torch.max(result), torch.min(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "description:   0%|                                                           | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.99e-01 | test_loss: 4.77e-01 | reg: 1.83e+01 | : 100%|█| 20/20 [00:15<00:00,  1.29it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "saving model version 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 9.20e-02 | test_loss: 7.91e-01 | reg: 1.84e+01 | : 100%|█| 20/20 [00:16<00:00,  1.22it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [array(0.32597438, dtype=float32),\n",
       "  array(0.2725942, dtype=float32),\n",
       "  array(0.24034075, dtype=float32),\n",
       "  array(0.2174355, dtype=float32),\n",
       "  array(0.1950771, dtype=float32),\n",
       "  array(0.1905998, dtype=float32),\n",
       "  array(0.17406127, dtype=float32),\n",
       "  array(0.1593095, dtype=float32),\n",
       "  array(0.14827032, dtype=float32),\n",
       "  array(0.13964488, dtype=float32),\n",
       "  array(0.13444753, dtype=float32),\n",
       "  array(0.12664321, dtype=float32),\n",
       "  array(0.1207858, dtype=float32),\n",
       "  array(0.11449644, dtype=float32),\n",
       "  array(0.10903602, dtype=float32),\n",
       "  array(0.10742125, dtype=float32),\n",
       "  array(0.10171282, dtype=float32),\n",
       "  array(0.09799804, dtype=float32),\n",
       "  array(0.09469176, dtype=float32),\n",
       "  array(0.09203254, dtype=float32)],\n",
       " 'test_loss': [array(0.43219915, dtype=float32),\n",
       "  array(0.5329175, dtype=float32),\n",
       "  array(0.56647694, dtype=float32),\n",
       "  array(0.6129143, dtype=float32),\n",
       "  array(0.6457783, dtype=float32),\n",
       "  array(0.72175556, dtype=float32),\n",
       "  array(0.7397249, dtype=float32),\n",
       "  array(0.7694195, dtype=float32),\n",
       "  array(0.7997186, dtype=float32),\n",
       "  array(0.81746805, dtype=float32),\n",
       "  array(0.7373164, dtype=float32),\n",
       "  array(0.7552243, dtype=float32),\n",
       "  array(0.76777244, dtype=float32),\n",
       "  array(0.779543, dtype=float32),\n",
       "  array(0.78277, dtype=float32),\n",
       "  array(0.78141594, dtype=float32),\n",
       "  array(0.7845382, dtype=float32),\n",
       "  array(0.7865547, dtype=float32),\n",
       "  array(0.78582275, dtype=float32),\n",
       "  array(0.7908814, dtype=float32)],\n",
       " 'reg': [array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(0., dtype=float32),\n",
       "  array(18.4197, dtype=float32)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kan = KAN(width=[in_dim,4,2,1], grid=3, k=3, seed=2)\n",
    "dataset = create_dataset(f, n_var=in_dim, ranges = [0,1])\n",
    "model_kan.fit(dataset, opt=\"LBFGS\", steps=20)\n",
    "model_kan = model_kan.refine(20)\n",
    "model_kan.fit(dataset, opt=\"LBFGS\", steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 4\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=640, bias=True)\n",
      "  (1): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "4 2\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "2 1\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000002380FED2C10>\n",
      "new version\n",
      " Epoch [1/200], Loss: 1.4987693,Test_Loss: 0.4784868\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\Noise_KAN_vs_NN.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Noise_KAN_vs_NN.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(dataloader)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Noise_KAN_vs_NN.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dataloader_test \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_dataloader(f, in_dim\u001b[39m=\u001b[39min_dim, num_samples\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/Noise_KAN_vs_NN.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(dataloader \u001b[39m=\u001b[39;49m dataloader,dataloader_test \u001b[39m=\u001b[39;49m dataloader_test, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\Kan_NN.py:91\u001b[0m, in \u001b[0;36mNeural_Kan.fit\u001b[1;34m(self, dataloader, dataloader_test, epochs, lr, weight_decay)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[39mfor\u001b[39;00m models \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m     90\u001b[0m         models\u001b[39m.\u001b[39mmultiply_grad_masks()\n\u001b[1;32m---> 91\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep() \n\u001b[0;32m     92\u001b[0m test_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_loss(dataloader_test)\n\u001b[0;32m     93\u001b[0m \u001b[39m#scheduler.step()\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39m#scheduler.step(test_loss)\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39m#print()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     74\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\radam.py:122\u001b[0m, in \u001b[0;36mRAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    118\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    120\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, grads, exp_avgs, exp_avg_sqs, state_steps)\n\u001b[1;32m--> 122\u001b[0m     radam(\n\u001b[0;32m    123\u001b[0m         params_with_grad,\n\u001b[0;32m    124\u001b[0m         grads,\n\u001b[0;32m    125\u001b[0m         exp_avgs,\n\u001b[0;32m    126\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    127\u001b[0m         state_steps,\n\u001b[0;32m    128\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    129\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    130\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    131\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    132\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    133\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    134\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    135\u001b[0m         decoupled_weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdecoupled_weight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    136\u001b[0m         has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    139\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\radam.py:254\u001b[0m, in \u001b[0;36mradam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, state_steps, decoupled_weight_decay, foreach, differentiable, has_complex, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_radam\n\u001b[1;32m--> 254\u001b[0m func(\n\u001b[0;32m    255\u001b[0m     params,\n\u001b[0;32m    256\u001b[0m     grads,\n\u001b[0;32m    257\u001b[0m     exp_avgs,\n\u001b[0;32m    258\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    259\u001b[0m     state_steps,\n\u001b[0;32m    260\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    261\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    262\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    263\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    264\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    265\u001b[0m     decoupled_weight_decay\u001b[39m=\u001b[39;49mdecoupled_weight_decay,\n\u001b[0;32m    266\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    267\u001b[0m     has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[0;32m    268\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\radam.py:315\u001b[0m, in \u001b[0;36m_single_tensor_radam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, state_steps, beta1, beta2, lr, weight_decay, eps, differentiable, decoupled_weight_decay, has_complex)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    314\u001b[0m exp_avg\u001b[39m.\u001b[39mlerp_(grad, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m--> 315\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad, value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[0;32m    317\u001b[0m \u001b[39m# correcting bias for the first moving moment\u001b[39;00m\n\u001b[0;32m    318\u001b[0m bias_corrected_exp_avg \u001b[39m=\u001b[39m exp_avg \u001b[39m/\u001b[39m bias_correction1\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Neural_Kan(shape = [in_dim,4,2,1], h = [64])\n",
    "dataloader = model.get_dataloader(f, in_dim=in_dim, num_samples=4000, batch_size=8)\n",
    "print(dataloader)\n",
    "dataloader_test = model.get_dataloader(f, in_dim=in_dim, num_samples=10, batch_size=1)\n",
    "model.fit(dataloader = dataloader,dataloader_test = dataloader_test, epochs=200, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_kan.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing (0,0,0) with x, r2=0.9995760917663574, c=1\n",
      "fixing (0,1,0) with x, r2=0.9999150633811951, c=1\n",
      "fixing (1,0,0) with x, r2=0.9758377075195312, c=1\n",
      "saving model version 0.2\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{962 x_{1}}{125} + \\frac{307 x_{2}}{40} + \\frac{4}{47}$"
      ],
      "text/plain": [
       "962*x_1/125 + 307*x_2/40 + 4/47"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kan.auto_symbolic()\n",
    "sf = model_kan.symbolic_formula()[0][0]\n",
    "nsimplify(ex_round(ex_round(sf, 3),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  function  fitting r2    r2 loss  complexity  complexity loss  total loss\n",
      "0        x    0.999576 -11.170322           1                1   -1.434064\n",
      "1      cos    0.999753 -11.926939           2                2   -0.785388\n",
      "2      sin    0.999753 -11.926604           2                2   -0.785321\n",
      "3      x^2    0.999590 -11.215821           2                2   -0.643164\n",
      "4      exp    0.999589 -11.215207           2                2   -0.643041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('x',\n",
       " (<function kan.utils.<lambda>(x)>,\n",
       "  <function kan.utils.<lambda>(x)>,\n",
       "  1,\n",
       "  <function kan.utils.<lambda>(x, y_th)>),\n",
       " 0.9995760917663574,\n",
       " 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kan.suggest_symbolic(0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('QtAgg')  # Force use of TkAgg backend\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 4\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=160, bias=True)\n",
      "  (1): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=160, out_features=320, bias=True)\n",
      "  (4): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=320, out_features=160, bias=True)\n",
      "  (7): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU()\n",
      ")\n",
      "4 2\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU()\n",
      ")\n",
      "2 1\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "  (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "# Fixed exponents between 0 and 1\n",
    "    alpha = 0.5\n",
    "    beta = 0.7\n",
    "    gamma = 0.3\n",
    "    delta = 0.8\n",
    "    epsilon = 0.6\n",
    "    omega = 0.4\n",
    "    # First term: (x^alpha + (1 - x)^beta)^gamma\n",
    "    term1 = 10*torch.sum(x ** alpha, dim = 1)\n",
    "    term2 = 2*torch.sum((1.0001 - x) ** beta, dim = 1)\n",
    "    # Second term: (sin(2πx)^delta)^epsilon\n",
    "    term3 = torch.abs(torch.sin(2 * torch.pi * torch.sum(x**delta, dim = 1)))\n",
    "    term4= torch.sum(x**0.3, dim = 1)\n",
    "    result = (torch.abs((torch.sin(2 * torch.pi * (term1 ** .5)) + torch.cos(20 * torch.pi * (term2 ** .4))))**omega + torch.abs((term3**.67 + term4**0.1)))\n",
    "    return torch.reshape(result, [result.shape[0], 1])\n",
    "#def f(X):\n",
    "#    return torch.sum(X, dim=1, keepdim=True)\n",
    "in_dim = 10\n",
    "model = Neural_Kan(shape = [in_dim,4,2,1], h = [16,32,16])\n",
    "dataloader = model.get_dataloader(f, in_dim=in_dim, num_samples=1000, batch_size=32)\n",
    "dataloader_test = model.get_dataloader(f, in_dim=in_dim, num_samples=200, batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader 10 32\n",
      "10 4\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=640, bias=True)\n",
      "  (1): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "4 2\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "2 1\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "new version\n",
      " Epoch [1/150], Loss: 5.9053942,Test_Loss: 6.1181258\n",
      " Epoch [2/150], Loss: 4.7583509,Test_Loss: 4.0702513\n",
      " Epoch [3/150], Loss: 3.6703599,Test_Loss: 2.9275390\n",
      " Epoch [4/150], Loss: 2.6199118,Test_Loss: 2.2093731\n",
      " Epoch [5/150], Loss: 1.9297934,Test_Loss: 1.5676552\n",
      " Epoch [6/150], Loss: 1.4359990,Test_Loss: 1.1654775\n",
      " Epoch [7/150], Loss: 1.1850268,Test_Loss: 0.9974595\n",
      " Epoch [8/150], Loss: 1.0025997,Test_Loss: 0.8860075\n",
      " Epoch [9/150], Loss: 0.7912419,Test_Loss: 0.8099503\n",
      " Epoch [10/150], Loss: 0.7839270,Test_Loss: 0.6589715\n",
      " Epoch [11/150], Loss: 0.6457510,Test_Loss: 0.6919171\n",
      " Epoch [12/150], Loss: 0.5529205,Test_Loss: 0.5959795\n",
      " Epoch [13/150], Loss: 0.4195294,Test_Loss: 0.4931920\n",
      " Epoch [14/150], Loss: 0.3878104,Test_Loss: 0.4872202\n",
      " Epoch [15/150], Loss: 0.3423204,Test_Loss: 0.5344226\n",
      " Epoch [16/150], Loss: 0.3197429,Test_Loss: 0.4581510\n",
      " Epoch [17/150], Loss: 0.3290699,Test_Loss: 0.3767960\n",
      " Epoch [18/150], Loss: 0.3472006,Test_Loss: 0.4228948\n",
      " Epoch [19/150], Loss: 0.3457815,Test_Loss: 0.4321424\n",
      " Epoch [20/150], Loss: 0.3221350,Test_Loss: 0.3605421\n",
      " Epoch [21/150], Loss: 0.3266519,Test_Loss: 0.4680165\n",
      " Epoch [22/150], Loss: 0.3064700,Test_Loss: 0.5580618\n",
      " Epoch [23/150], Loss: 0.3088373,Test_Loss: 0.4462205\n",
      " Epoch [24/150], Loss: 0.3035961,Test_Loss: 0.3892048\n",
      " Epoch [25/150], Loss: 0.2924826,Test_Loss: 0.4148272\n",
      " Epoch [26/150], Loss: 0.2736410,Test_Loss: 0.3761838\n",
      " Epoch [27/150], Loss: 0.2960454,Test_Loss: 0.4231962\n",
      " Epoch [28/150], Loss: 0.2570956,Test_Loss: 0.5665568\n",
      " Epoch [29/150], Loss: 0.3006156,Test_Loss: 0.5000995\n",
      " Epoch [30/150], Loss: 0.2771644,Test_Loss: 0.4506111\n",
      " Epoch [31/150], Loss: 0.2564681,Test_Loss: 0.3863811\n",
      " Epoch [32/150], Loss: 0.2521091,Test_Loss: 0.3144546\n",
      " Epoch [33/150], Loss: 0.2621003,Test_Loss: 0.4449926\n",
      " Epoch [34/150], Loss: 0.2681427,Test_Loss: 0.5542731\n",
      " Epoch [35/150], Loss: 0.2756625,Test_Loss: 0.5817393\n",
      " Epoch [36/150], Loss: 0.2757427,Test_Loss: 0.4971232\n",
      " Epoch [37/150], Loss: 0.2486859,Test_Loss: 0.2668972\n",
      " Epoch [38/150], Loss: 0.2395761,Test_Loss: 0.5058170\n",
      " Epoch [39/150], Loss: 0.2478964,Test_Loss: 0.3023869\n",
      " Epoch [40/150], Loss: 0.2386486,Test_Loss: 0.2724104\n",
      " Epoch [41/150], Loss: 0.2408300,Test_Loss: 0.3093434\n",
      " Epoch [42/150], Loss: 0.2362553,Test_Loss: 0.3701661\n",
      " Epoch [43/150], Loss: 0.2160521,Test_Loss: 0.3652934\n",
      " Epoch [44/150], Loss: 0.2097974,Test_Loss: 0.3345495\n",
      " Epoch [45/150], Loss: 0.2163957,Test_Loss: 0.3445590\n",
      " Epoch [46/150], Loss: 0.2122827,Test_Loss: 0.3331577\n",
      " Epoch [47/150], Loss: 0.2205605,Test_Loss: 0.2994269\n",
      " Epoch [48/150], Loss: 0.2046448,Test_Loss: 0.2755363\n",
      " Epoch [49/150], Loss: 0.2165186,Test_Loss: 0.2882029\n",
      " Epoch [50/150], Loss: 0.2047334,Test_Loss: 0.2515135\n",
      " Epoch [51/150], Loss: 0.1959834,Test_Loss: 0.3754051\n",
      " Epoch [52/150], Loss: 0.2094336,Test_Loss: 0.2491617\n",
      " Epoch [53/150], Loss: 0.1942090,Test_Loss: 0.2413508\n",
      " Epoch [54/150], Loss: 0.1846239,Test_Loss: 0.2504720\n",
      " Epoch [55/150], Loss: 0.1946895,Test_Loss: 0.2406427\n",
      " Epoch [56/150], Loss: 0.1984704,Test_Loss: 0.2226155\n",
      " Epoch [57/150], Loss: 0.1980199,Test_Loss: 0.2270749\n",
      " Epoch [58/150], Loss: 0.1836775,Test_Loss: 0.2408001\n",
      " Epoch [59/150], Loss: 0.1754090,Test_Loss: 0.2110334\n",
      " Epoch [60/150], Loss: 0.1767018,Test_Loss: 0.2221272\n",
      " Epoch [61/150], Loss: 0.1718505,Test_Loss: 0.2141650\n",
      " Epoch [62/150], Loss: 0.1685656,Test_Loss: 0.2015523\n",
      " Epoch [63/150], Loss: 0.1645429,Test_Loss: 0.1965973\n",
      " Epoch [64/150], Loss: 0.1584305,Test_Loss: 0.2047218\n",
      " Epoch [65/150], Loss: 0.1635190,Test_Loss: 0.1960880\n",
      " Epoch [66/150], Loss: 0.1641997,Test_Loss: 0.1930740\n",
      " Epoch [67/150], Loss: 0.1588559,Test_Loss: 0.1845190\n",
      " Epoch [68/150], Loss: 0.1605039,Test_Loss: 0.1841434\n",
      " Epoch [69/150], Loss: 0.1630766,Test_Loss: 0.1813937\n",
      " Epoch [70/150], Loss: 0.1587911,Test_Loss: 0.1786257\n",
      " Epoch [71/150], Loss: 0.1546393,Test_Loss: 0.1806377\n",
      " Epoch [72/150], Loss: 0.1586828,Test_Loss: 0.1818746\n",
      " Epoch [73/150], Loss: 0.1563728,Test_Loss: 0.1821637\n",
      " Epoch [74/150], Loss: 0.1591540,Test_Loss: 0.1807820\n",
      " Epoch [75/150], Loss: 0.1586170,Test_Loss: 0.1795232\n",
      " Epoch [76/150], Loss: 0.1579499,Test_Loss: 0.1815286\n",
      " Epoch [77/150], Loss: 0.1599870,Test_Loss: 0.1801132\n",
      " Epoch [78/150], Loss: 0.1586453,Test_Loss: 0.1780238\n",
      " Epoch [79/150], Loss: 0.1587048,Test_Loss: 0.1768938\n",
      " Epoch [80/150], Loss: 0.1593418,Test_Loss: 0.1875957\n",
      " Epoch [81/150], Loss: 0.1586263,Test_Loss: 0.1808246\n",
      " Epoch [82/150], Loss: 0.1622408,Test_Loss: 0.1815357\n",
      " Epoch [83/150], Loss: 0.1622566,Test_Loss: 0.1749069\n",
      " Epoch [84/150], Loss: 0.1580843,Test_Loss: 0.1825764\n",
      " Epoch [85/150], Loss: 0.1630269,Test_Loss: 0.1783955\n",
      " Epoch [86/150], Loss: 0.1581653,Test_Loss: 0.1796886\n",
      " Epoch [87/150], Loss: 0.1601276,Test_Loss: 0.1793754\n",
      " Epoch [88/150], Loss: 0.1588009,Test_Loss: 0.1779509\n",
      " Epoch [89/150], Loss: 0.1616111,Test_Loss: 0.1757383\n",
      " Epoch [90/150], Loss: 0.1584445,Test_Loss: 0.1766609\n",
      " Epoch [91/150], Loss: 0.1604924,Test_Loss: 0.1786792\n",
      " Epoch [92/150], Loss: 0.1605024,Test_Loss: 0.1749230\n",
      " Epoch [93/150], Loss: 0.1586413,Test_Loss: 0.1787451\n",
      " Epoch [94/150], Loss: 0.1597095,Test_Loss: 0.1763445\n",
      " Epoch [95/150], Loss: 0.1594671,Test_Loss: 0.1895823\n",
      " Epoch [96/150], Loss: 0.1597631,Test_Loss: 0.1798903\n",
      " Epoch [97/150], Loss: 0.1594753,Test_Loss: 0.1767284\n",
      " Epoch [98/150], Loss: 0.1564081,Test_Loss: 0.1792811\n",
      " Epoch [99/150], Loss: 0.1582658,Test_Loss: 0.1792017\n",
      " Epoch [100/150], Loss: 0.1595240,Test_Loss: 0.1774640\n",
      " Epoch [101/150], Loss: 0.1586912,Test_Loss: 0.1809932\n",
      " Epoch [102/150], Loss: 0.1580903,Test_Loss: 0.1800348\n",
      " Epoch [103/150], Loss: 0.1602343,Test_Loss: 0.1779087\n",
      " Epoch [104/150], Loss: 0.1580105,Test_Loss: 0.1771865\n",
      " Epoch [105/150], Loss: 0.1600233,Test_Loss: 0.1777694\n",
      " Epoch [106/150], Loss: 0.1622669,Test_Loss: 0.1773131\n",
      " Epoch [107/150], Loss: 0.1604575,Test_Loss: 0.1825262\n",
      " Epoch [108/150], Loss: 0.1581450,Test_Loss: 0.1792473\n",
      " Epoch [109/150], Loss: 0.1574806,Test_Loss: 0.1802821\n",
      " Epoch [110/150], Loss: 0.1594222,Test_Loss: 0.1808445\n",
      " Epoch [111/150], Loss: 0.1612543,Test_Loss: 0.1812989\n",
      " Epoch [112/150], Loss: 0.1595441,Test_Loss: 0.1770880\n",
      " Epoch [113/150], Loss: 0.1620003,Test_Loss: 0.1815594\n",
      " Epoch [114/150], Loss: 0.1584792,Test_Loss: 0.1781593\n",
      " Epoch [115/150], Loss: 0.1613330,Test_Loss: 0.1760230\n",
      " Epoch [116/150], Loss: 0.1577270,Test_Loss: 0.1793797\n",
      " Epoch [117/150], Loss: 0.1582361,Test_Loss: 0.1772360\n",
      " Epoch [118/150], Loss: 0.1615270,Test_Loss: 0.1765606\n",
      " Epoch [119/150], Loss: 0.1586922,Test_Loss: 0.1825998\n",
      " Epoch [120/150], Loss: 0.1602145,Test_Loss: 0.1742329\n",
      " Epoch [121/150], Loss: 0.1602229,Test_Loss: 0.1794873\n",
      " Epoch [122/150], Loss: 0.1565841,Test_Loss: 0.1777712\n",
      " Epoch [123/150], Loss: 0.1587766,Test_Loss: 0.1777039\n",
      " Epoch [124/150], Loss: 0.1573807,Test_Loss: 0.1807476\n",
      " Epoch [125/150], Loss: 0.1605023,Test_Loss: 0.1772166\n",
      " Epoch [126/150], Loss: 0.1572351,Test_Loss: 0.1790995\n",
      " Epoch [127/150], Loss: 0.1620211,Test_Loss: 0.1817887\n",
      " Epoch [128/150], Loss: 0.1576871,Test_Loss: 0.1790770\n",
      " Epoch [129/150], Loss: 0.1591991,Test_Loss: 0.1786667\n",
      " Epoch [130/150], Loss: 0.1597819,Test_Loss: 0.1789623\n",
      " Epoch [131/150], Loss: 0.1592814,Test_Loss: 0.1768710\n",
      " Epoch [132/150], Loss: 0.1587106,Test_Loss: 0.1800107\n",
      " Epoch [133/150], Loss: 0.1574207,Test_Loss: 0.1758220\n",
      " Epoch [134/150], Loss: 0.1587116,Test_Loss: 0.1778275\n",
      " Epoch [135/150], Loss: 0.1580737,Test_Loss: 0.1824558\n",
      " Epoch [136/150], Loss: 0.1592884,Test_Loss: 0.1812466\n",
      " Epoch [137/150], Loss: 0.1599207,Test_Loss: 0.1800823\n",
      " Epoch [138/150], Loss: 0.1618197,Test_Loss: 0.1846819\n",
      " Epoch [139/150], Loss: 0.1579736,Test_Loss: 0.1789086\n",
      " Epoch [140/150], Loss: 0.1594474,Test_Loss: 0.1794163\n",
      " Epoch [141/150], Loss: 0.1589825,Test_Loss: 0.1851596\n",
      " Epoch [142/150], Loss: 0.1583374,Test_Loss: 0.1772652\n",
      " Epoch [143/150], Loss: 0.1612035,Test_Loss: 0.1802390\n",
      " Epoch [144/150], Loss: 0.1582078,Test_Loss: 0.1789528\n",
      " Epoch [145/150], Loss: 0.1606133,Test_Loss: 0.1790548\n",
      " Epoch [146/150], Loss: 0.1594108,Test_Loss: 0.1756160\n",
      " Epoch [147/150], Loss: 0.1602406,Test_Loss: 0.1822572\n",
      " Epoch [148/150], Loss: 0.1601158,Test_Loss: 0.1824672\n",
      " Epoch [149/150], Loss: 0.1604678,Test_Loss: 0.1764935\n",
      " Epoch [150/150], Loss: 0.1599645,Test_Loss: 0.1830527\n",
      "10 4\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=640, bias=True)\n",
      "  (1): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "4 2\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "2 1\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "new version\n",
      " Epoch [1/150], Loss: 5.5101983,Test_Loss: 6.3808790\n",
      " Epoch [2/150], Loss: 4.4470840,Test_Loss: 4.3095357\n",
      " Epoch [3/150], Loss: 3.3896058,Test_Loss: 2.8363629\n",
      " Epoch [4/150], Loss: 2.4175149,Test_Loss: 1.9222705\n",
      " Epoch [5/150], Loss: 1.6744148,Test_Loss: 1.5709424\n",
      " Epoch [6/150], Loss: 1.1356960,Test_Loss: 1.1256933\n",
      " Epoch [7/150], Loss: 0.8894908,Test_Loss: 0.9573887\n",
      " Epoch [8/150], Loss: 0.7799143,Test_Loss: 0.8470839\n",
      " Epoch [9/150], Loss: 0.6654324,Test_Loss: 0.8121142\n",
      " Epoch [10/150], Loss: 0.6293916,Test_Loss: 0.7150137\n",
      " Epoch [11/150], Loss: 0.5932883,Test_Loss: 0.6885644\n",
      " Epoch [12/150], Loss: 0.5512692,Test_Loss: 0.6676832\n",
      " Epoch [13/150], Loss: 0.5046360,Test_Loss: 0.6480132\n",
      " Epoch [14/150], Loss: 0.5791617,Test_Loss: 0.6487356\n",
      " Epoch [15/150], Loss: 0.4879043,Test_Loss: 0.6170248\n",
      " Epoch [16/150], Loss: 0.4965860,Test_Loss: 0.5634945\n",
      " Epoch [17/150], Loss: 0.3937053,Test_Loss: 0.5023814\n",
      " Epoch [18/150], Loss: 0.4377991,Test_Loss: 0.4466692\n",
      " Epoch [19/150], Loss: 0.3670418,Test_Loss: 0.4304409\n",
      " Epoch [20/150], Loss: 0.3406703,Test_Loss: 0.3620918\n",
      " Epoch [21/150], Loss: 0.3386917,Test_Loss: 0.3885710\n",
      " Epoch [22/150], Loss: 0.3348585,Test_Loss: 0.3601341\n",
      " Epoch [23/150], Loss: 0.3017242,Test_Loss: 0.3542018\n",
      " Epoch [24/150], Loss: 0.3210154,Test_Loss: 0.3708611\n",
      " Epoch [25/150], Loss: 0.2881061,Test_Loss: 0.3647307\n",
      " Epoch [26/150], Loss: 0.2829886,Test_Loss: 0.3892432\n",
      " Epoch [27/150], Loss: 0.2760764,Test_Loss: 0.3145107\n",
      " Epoch [28/150], Loss: 0.2907212,Test_Loss: 0.3475721\n",
      " Epoch [29/150], Loss: 0.2746928,Test_Loss: 0.3546688\n",
      " Epoch [30/150], Loss: 0.2916886,Test_Loss: 0.3298624\n",
      " Epoch [31/150], Loss: 0.2685358,Test_Loss: 0.2941212\n",
      " Epoch [32/150], Loss: 0.2728747,Test_Loss: 0.3463685\n",
      " Epoch [33/150], Loss: 0.2646473,Test_Loss: 0.3191174\n",
      " Epoch [34/150], Loss: 0.2584475,Test_Loss: 0.3368119\n",
      " Epoch [35/150], Loss: 0.2556558,Test_Loss: 0.3467880\n",
      " Epoch [36/150], Loss: 0.2613893,Test_Loss: 0.3502868\n",
      " Epoch [37/150], Loss: 0.2582310,Test_Loss: 0.3303697\n",
      " Epoch [38/150], Loss: 0.2776648,Test_Loss: 0.2977827\n",
      " Epoch [39/150], Loss: 0.2469242,Test_Loss: 0.2984287\n",
      " Epoch [40/150], Loss: 0.2695382,Test_Loss: 0.2963597\n",
      " Epoch [41/150], Loss: 0.2588978,Test_Loss: 0.3284175\n",
      " Epoch [42/150], Loss: 0.2616555,Test_Loss: 0.3070340\n",
      " Epoch [43/150], Loss: 0.2537254,Test_Loss: 0.3094180\n",
      " Epoch [44/150], Loss: 0.2351877,Test_Loss: 0.2913472\n",
      " Epoch [45/150], Loss: 0.2511613,Test_Loss: 0.2897126\n",
      " Epoch [46/150], Loss: 0.2324116,Test_Loss: 0.2715948\n",
      " Epoch [47/150], Loss: 0.2340164,Test_Loss: 0.2916319\n",
      " Epoch [48/150], Loss: 0.2479499,Test_Loss: 0.2502523\n",
      " Epoch [49/150], Loss: 0.2224189,Test_Loss: 0.2707126\n",
      " Epoch [50/150], Loss: 0.2191571,Test_Loss: 0.2761462\n",
      " Epoch [51/150], Loss: 0.2309785,Test_Loss: 0.2682259\n",
      " Epoch [52/150], Loss: 0.2289511,Test_Loss: 0.2837113\n",
      " Epoch [53/150], Loss: 0.2190479,Test_Loss: 0.2698266\n",
      " Epoch [54/150], Loss: 0.2210421,Test_Loss: 0.2654218\n",
      " Epoch [55/150], Loss: 0.2219761,Test_Loss: 0.2626103\n",
      " Epoch [56/150], Loss: 0.2256999,Test_Loss: 0.2538792\n",
      " Epoch [57/150], Loss: 0.2112235,Test_Loss: 0.2374449\n",
      " Epoch [58/150], Loss: 0.2192919,Test_Loss: 0.2519175\n",
      " Epoch [59/150], Loss: 0.2166975,Test_Loss: 0.2489221\n",
      " Epoch [60/150], Loss: 0.2168124,Test_Loss: 0.2444718\n",
      " Epoch [61/150], Loss: 0.2143813,Test_Loss: 0.2467178\n",
      " Epoch [62/150], Loss: 0.1977630,Test_Loss: 0.2428074\n",
      " Epoch [63/150], Loss: 0.2081825,Test_Loss: 0.2490731\n",
      " Epoch [64/150], Loss: 0.1936835,Test_Loss: 0.2368371\n",
      " Epoch [65/150], Loss: 0.1958539,Test_Loss: 0.2370819\n",
      " Epoch [66/150], Loss: 0.1803874,Test_Loss: 0.2286826\n",
      " Epoch [67/150], Loss: 0.1970787,Test_Loss: 0.2352980\n",
      " Epoch [68/150], Loss: 0.1929536,Test_Loss: 0.2336173\n",
      " Epoch [69/150], Loss: 0.1848261,Test_Loss: 0.2347752\n",
      " Epoch [70/150], Loss: 0.1903357,Test_Loss: 0.2344087\n",
      " Epoch [71/150], Loss: 0.1821782,Test_Loss: 0.2336707\n",
      " Epoch [72/150], Loss: 0.1936998,Test_Loss: 0.2250724\n",
      " Epoch [73/150], Loss: 0.1917322,Test_Loss: 0.2144251\n",
      " Epoch [74/150], Loss: 0.1923297,Test_Loss: 0.2328632\n",
      " Epoch [75/150], Loss: 0.1802268,Test_Loss: 0.2191971\n",
      " Epoch [76/150], Loss: 0.1756512,Test_Loss: 0.2402701\n",
      " Epoch [77/150], Loss: 0.1921150,Test_Loss: 0.2251442\n",
      " Epoch [78/150], Loss: 0.1777771,Test_Loss: 0.2198825\n",
      " Epoch [79/150], Loss: 0.1775152,Test_Loss: 0.2156766\n",
      " Epoch [80/150], Loss: 0.1738582,Test_Loss: 0.2153609\n",
      " Epoch [81/150], Loss: 0.1712929,Test_Loss: 0.2260782\n",
      " Epoch [82/150], Loss: 0.1773108,Test_Loss: 0.2065770\n",
      " Epoch [83/150], Loss: 0.1699122,Test_Loss: 0.2314769\n",
      " Epoch [84/150], Loss: 0.1771539,Test_Loss: 0.2076532\n",
      " Epoch [85/150], Loss: 0.1750234,Test_Loss: 0.2101980\n",
      " Epoch [86/150], Loss: 0.1662002,Test_Loss: 0.2135839\n",
      " Epoch [87/150], Loss: 0.1710293,Test_Loss: 0.2059789\n",
      " Epoch [88/150], Loss: 0.1666555,Test_Loss: 0.2014095\n",
      " Epoch [89/150], Loss: 0.1705519,Test_Loss: 0.2041906\n",
      " Epoch [90/150], Loss: 0.1702687,Test_Loss: 0.2427616\n",
      " Epoch [91/150], Loss: 0.1600804,Test_Loss: 0.2018706\n",
      " Epoch [92/150], Loss: 0.1698441,Test_Loss: 0.2039387\n",
      " Epoch [93/150], Loss: 0.1629936,Test_Loss: 0.2071459\n",
      " Epoch [94/150], Loss: 0.1649566,Test_Loss: 0.2076744\n",
      " Epoch [95/150], Loss: 0.1601915,Test_Loss: 0.2122303\n",
      " Epoch [96/150], Loss: 0.1684793,Test_Loss: 0.2088425\n",
      " Epoch [97/150], Loss: 0.1615669,Test_Loss: 0.2020536\n",
      " Epoch [98/150], Loss: 0.1652124,Test_Loss: 0.2082935\n",
      " Epoch [99/150], Loss: 0.1697075,Test_Loss: 0.2059668\n",
      " Epoch [100/150], Loss: 0.1606884,Test_Loss: 0.1985397\n",
      " Epoch [101/150], Loss: 0.1600190,Test_Loss: 0.1993041\n",
      " Epoch [102/150], Loss: 0.1628035,Test_Loss: 0.1925120\n",
      " Epoch [103/150], Loss: 0.1622894,Test_Loss: 0.1940230\n",
      " Epoch [104/150], Loss: 0.1576140,Test_Loss: 0.1920816\n",
      " Epoch [105/150], Loss: 0.1549177,Test_Loss: 0.1915548\n",
      " Epoch [106/150], Loss: 0.1553331,Test_Loss: 0.1926994\n",
      " Epoch [107/150], Loss: 0.1598464,Test_Loss: 0.1946485\n",
      " Epoch [108/150], Loss: 0.1576058,Test_Loss: 0.1913954\n",
      " Epoch [109/150], Loss: 0.1582043,Test_Loss: 0.1970242\n",
      " Epoch [110/150], Loss: 0.1555405,Test_Loss: 0.2064137\n",
      " Epoch [111/150], Loss: 0.1573657,Test_Loss: 0.1923602\n",
      " Epoch [112/150], Loss: 0.1615843,Test_Loss: 0.2010837\n",
      " Epoch [113/150], Loss: 0.1571548,Test_Loss: 0.2129389\n",
      " Epoch [114/150], Loss: 0.1582278,Test_Loss: 0.2216104\n",
      " Epoch [115/150], Loss: 0.1553346,Test_Loss: 0.1928650\n",
      " Epoch [116/150], Loss: 0.1669226,Test_Loss: 0.1853951\n",
      " Epoch [117/150], Loss: 0.1592283,Test_Loss: 0.1965885\n",
      " Epoch [118/150], Loss: 0.1549466,Test_Loss: 0.1986700\n",
      " Epoch [119/150], Loss: 0.1599973,Test_Loss: 0.1979923\n",
      " Epoch [120/150], Loss: 0.1574860,Test_Loss: 0.1932350\n",
      " Epoch [121/150], Loss: 0.1582499,Test_Loss: 0.1910777\n",
      " Epoch [122/150], Loss: 0.1564152,Test_Loss: 0.2022483\n",
      " Epoch [123/150], Loss: 0.1577034,Test_Loss: 0.1879715\n",
      " Epoch [124/150], Loss: 0.1630160,Test_Loss: 0.1833008\n",
      " Epoch [125/150], Loss: 0.1552669,Test_Loss: 0.1881251\n",
      " Epoch [126/150], Loss: 0.1533316,Test_Loss: 0.1876721\n",
      " Epoch [127/150], Loss: 0.1533216,Test_Loss: 0.1751841\n",
      " Epoch [128/150], Loss: 0.1562573,Test_Loss: 0.1709578\n",
      " Epoch [129/150], Loss: 0.1543040,Test_Loss: 0.1861354\n",
      " Epoch [130/150], Loss: 0.1556161,Test_Loss: 0.1947718\n",
      " Epoch [131/150], Loss: 0.1553073,Test_Loss: 0.2000559\n",
      " Epoch [132/150], Loss: 0.1640982,Test_Loss: 0.1869178\n",
      " Epoch [133/150], Loss: 0.1525792,Test_Loss: 0.1797303\n",
      " Epoch [134/150], Loss: 0.1560137,Test_Loss: 0.1827557\n",
      " Epoch [135/150], Loss: 0.1512885,Test_Loss: 0.1869185\n",
      " Epoch [136/150], Loss: 0.1583615,Test_Loss: 0.1818367\n",
      " Epoch [137/150], Loss: 0.1572790,Test_Loss: 0.1841136\n",
      " Epoch [138/150], Loss: 0.1564448,Test_Loss: 0.1863174\n",
      " Epoch [139/150], Loss: 0.1548296,Test_Loss: 0.1856342\n",
      " Epoch [140/150], Loss: 0.1566319,Test_Loss: 0.1910715\n",
      " Epoch [141/150], Loss: 0.1577106,Test_Loss: 0.1869616\n",
      " Epoch [142/150], Loss: 0.1511967,Test_Loss: 0.1810623\n",
      " Epoch [143/150], Loss: 0.1555060,Test_Loss: 0.1811846\n",
      " Epoch [144/150], Loss: 0.1496060,Test_Loss: 0.1857308\n",
      " Epoch [145/150], Loss: 0.1519519,Test_Loss: 0.1943454\n",
      " Epoch [146/150], Loss: 0.1522880,Test_Loss: 0.2007558\n",
      " Epoch [147/150], Loss: 0.1501561,Test_Loss: 0.1856064\n",
      " Epoch [148/150], Loss: 0.1508157,Test_Loss: 0.1876015\n",
      " Epoch [149/150], Loss: 0.1505544,Test_Loss: 0.1794658\n",
      " Epoch [150/150], Loss: 0.1566911,Test_Loss: 0.1766152\n",
      "10 4\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=640, bias=True)\n",
      "  (1): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "4 2\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "2 1\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "new version\n",
      " Epoch [1/150], Loss: 4.5708134,Test_Loss: 5.0796036\n",
      " Epoch [2/150], Loss: 3.6401478,Test_Loss: 3.5688647\n",
      " Epoch [3/150], Loss: 2.6688996,Test_Loss: 2.5807780\n",
      " Epoch [4/150], Loss: 1.8660648,Test_Loss: 1.6400867\n",
      " Epoch [5/150], Loss: 1.2300892,Test_Loss: 1.1619897\n",
      " Epoch [6/150], Loss: 0.9315642,Test_Loss: 0.6812021\n",
      " Epoch [7/150], Loss: 0.7622083,Test_Loss: 0.5805357\n",
      " Epoch [8/150], Loss: 0.7262647,Test_Loss: 0.5941277\n",
      " Epoch [9/150], Loss: 0.5958506,Test_Loss: 0.5141909\n",
      " Epoch [10/150], Loss: 0.5008065,Test_Loss: 0.3999437\n",
      " Epoch [11/150], Loss: 0.4607535,Test_Loss: 0.4124757\n",
      " Epoch [12/150], Loss: 0.3907754,Test_Loss: 0.3675419\n",
      " Epoch [13/150], Loss: 0.3717879,Test_Loss: 0.3200254\n",
      " Epoch [14/150], Loss: 0.3240431,Test_Loss: 0.2775636\n",
      " Epoch [15/150], Loss: 0.3045941,Test_Loss: 0.2946592\n",
      " Epoch [16/150], Loss: 0.3404610,Test_Loss: 0.2467068\n",
      " Epoch [17/150], Loss: 0.3169765,Test_Loss: 0.3025123\n",
      " Epoch [18/150], Loss: 0.2672199,Test_Loss: 0.2875459\n",
      " Epoch [19/150], Loss: 0.2848749,Test_Loss: 0.2342804\n",
      " Epoch [20/150], Loss: 0.2782137,Test_Loss: 0.2907289\n",
      " Epoch [21/150], Loss: 0.2854266,Test_Loss: 0.2514843\n",
      " Epoch [22/150], Loss: 0.3001006,Test_Loss: 0.3249542\n",
      " Epoch [23/150], Loss: 0.2928067,Test_Loss: 0.2717430\n",
      " Epoch [24/150], Loss: 0.2963585,Test_Loss: 0.2516817\n",
      " Epoch [25/150], Loss: 0.2791342,Test_Loss: 0.2453326\n",
      " Epoch [26/150], Loss: 0.2585216,Test_Loss: 0.2398663\n",
      " Epoch [27/150], Loss: 0.2566337,Test_Loss: 0.2477619\n",
      " Epoch [28/150], Loss: 0.2591966,Test_Loss: 0.2673992\n",
      " Epoch [29/150], Loss: 0.2792350,Test_Loss: 0.2867134\n",
      " Epoch [30/150], Loss: 0.2553759,Test_Loss: 0.3058114\n",
      " Epoch [31/150], Loss: 0.2700878,Test_Loss: 0.2867026\n",
      " Epoch [32/150], Loss: 0.2670740,Test_Loss: 0.2681910\n",
      " Epoch [33/150], Loss: 0.2600528,Test_Loss: 0.2173774\n",
      " Epoch [34/150], Loss: 0.2574430,Test_Loss: 0.2701814\n",
      " Epoch [35/150], Loss: 0.2470489,Test_Loss: 0.2397845\n",
      " Epoch [36/150], Loss: 0.2539269,Test_Loss: 0.3899561\n",
      " Epoch [37/150], Loss: 0.2488041,Test_Loss: 0.2452467\n",
      " Epoch [38/150], Loss: 0.2245559,Test_Loss: 0.2604165\n",
      " Epoch [39/150], Loss: 0.2473113,Test_Loss: 0.2471166\n",
      " Epoch [40/150], Loss: 0.2339199,Test_Loss: 0.3083183\n",
      " Epoch [41/150], Loss: 0.2313860,Test_Loss: 0.2355125\n",
      " Epoch [42/150], Loss: 0.2337072,Test_Loss: 0.3094567\n",
      " Epoch [43/150], Loss: 0.2061439,Test_Loss: 0.2390084\n",
      " Epoch [44/150], Loss: 0.2185668,Test_Loss: 0.2569501\n",
      " Epoch [45/150], Loss: 0.2313595,Test_Loss: 0.2628444\n",
      " Epoch [46/150], Loss: 0.2311582,Test_Loss: 0.2104090\n",
      " Epoch [47/150], Loss: 0.2230330,Test_Loss: 0.2607277\n",
      " Epoch [48/150], Loss: 0.2204394,Test_Loss: 0.2433295\n",
      " Epoch [49/150], Loss: 0.2151966,Test_Loss: 0.2196740\n",
      " Epoch [50/150], Loss: 0.2276364,Test_Loss: 0.2186715\n",
      " Epoch [51/150], Loss: 0.2096289,Test_Loss: 0.2258215\n",
      " Epoch [52/150], Loss: 0.2239548,Test_Loss: 0.2535699\n",
      " Epoch [53/150], Loss: 0.2209637,Test_Loss: 0.2033990\n",
      " Epoch [54/150], Loss: 0.2262830,Test_Loss: 0.2433856\n",
      " Epoch [55/150], Loss: 0.2281970,Test_Loss: 0.1968163\n",
      " Epoch [56/150], Loss: 0.2161301,Test_Loss: 0.2255193\n",
      " Epoch [57/150], Loss: 0.2325355,Test_Loss: 0.2759773\n",
      " Epoch [58/150], Loss: 0.2186167,Test_Loss: 0.2292185\n",
      " Epoch [59/150], Loss: 0.2108755,Test_Loss: 0.2224023\n",
      " Epoch [60/150], Loss: 0.2184479,Test_Loss: 0.2290938\n",
      " Epoch [61/150], Loss: 0.2002797,Test_Loss: 0.2502152\n",
      " Epoch [62/150], Loss: 0.2089129,Test_Loss: 0.2188173\n",
      " Epoch [63/150], Loss: 0.2076202,Test_Loss: 0.2217748\n",
      " Epoch [64/150], Loss: 0.1918155,Test_Loss: 0.2061333\n",
      " Epoch [65/150], Loss: 0.1923269,Test_Loss: 0.2183163\n",
      " Epoch [66/150], Loss: 0.2003489,Test_Loss: 0.2241979\n",
      " Epoch [67/150], Loss: 0.2064935,Test_Loss: 0.2213338\n",
      " Epoch [68/150], Loss: 0.2074511,Test_Loss: 0.2224342\n",
      " Epoch [69/150], Loss: 0.1932492,Test_Loss: 0.1858917\n",
      " Epoch [70/150], Loss: 0.2028814,Test_Loss: 0.1984454\n",
      " Epoch [71/150], Loss: 0.1967322,Test_Loss: 0.2105714\n",
      " Epoch [72/150], Loss: 0.1954861,Test_Loss: 0.2168541\n",
      " Epoch [73/150], Loss: 0.1931557,Test_Loss: 0.1916896\n",
      " Epoch [74/150], Loss: 0.1840846,Test_Loss: 0.2050608\n",
      " Epoch [75/150], Loss: 0.1857270,Test_Loss: 0.1766692\n",
      " Epoch [76/150], Loss: 0.1756504,Test_Loss: 0.1868880\n",
      " Epoch [77/150], Loss: 0.1788348,Test_Loss: 0.2310073\n",
      " Epoch [78/150], Loss: 0.1804029,Test_Loss: 0.2030574\n",
      " Epoch [79/150], Loss: 0.1731899,Test_Loss: 0.2021103\n",
      " Epoch [80/150], Loss: 0.1784441,Test_Loss: 0.2197920\n",
      " Epoch [81/150], Loss: 0.1826880,Test_Loss: 0.2035498\n",
      " Epoch [82/150], Loss: 0.1990700,Test_Loss: 0.2281986\n",
      " Epoch [83/150], Loss: 0.1726187,Test_Loss: 0.1773802\n",
      " Epoch [84/150], Loss: 0.1725135,Test_Loss: 0.2026758\n",
      " Epoch [85/150], Loss: 0.1821154,Test_Loss: 0.2087845\n",
      " Epoch [86/150], Loss: 0.1748280,Test_Loss: 0.2006361\n",
      " Epoch [87/150], Loss: 0.1821618,Test_Loss: 0.2029326\n",
      " Epoch [88/150], Loss: 0.1712385,Test_Loss: 0.1953874\n",
      " Epoch [89/150], Loss: 0.1715612,Test_Loss: 0.1766394\n",
      " Epoch [90/150], Loss: 0.1682765,Test_Loss: 0.1875052\n",
      " Epoch [91/150], Loss: 0.1802930,Test_Loss: 0.1850777\n",
      " Epoch [92/150], Loss: 0.1708090,Test_Loss: 0.1938676\n",
      " Epoch [93/150], Loss: 0.1769350,Test_Loss: 0.1763127\n",
      " Epoch [94/150], Loss: 0.1743425,Test_Loss: 0.1832465\n",
      " Epoch [95/150], Loss: 0.1670135,Test_Loss: 0.1943648\n",
      " Epoch [96/150], Loss: 0.1786228,Test_Loss: 0.1756858\n",
      " Epoch [97/150], Loss: 0.1747796,Test_Loss: 0.1791937\n",
      " Epoch [98/150], Loss: 0.1692116,Test_Loss: 0.1853358\n",
      " Epoch [99/150], Loss: 0.1618370,Test_Loss: 0.1844298\n",
      " Epoch [100/150], Loss: 0.1661406,Test_Loss: 0.1970565\n",
      " Epoch [101/150], Loss: 0.1616743,Test_Loss: 0.1806707\n",
      " Epoch [102/150], Loss: 0.1631572,Test_Loss: 0.1969189\n",
      " Epoch [103/150], Loss: 0.1627610,Test_Loss: 0.1873460\n",
      " Epoch [104/150], Loss: 0.1691873,Test_Loss: 0.1795717\n",
      " Epoch [105/150], Loss: 0.1606132,Test_Loss: 0.1703693\n",
      " Epoch [106/150], Loss: 0.1731837,Test_Loss: 0.1792937\n",
      " Epoch [107/150], Loss: 0.1555988,Test_Loss: 0.1866804\n",
      " Epoch [108/150], Loss: 0.1625514,Test_Loss: 0.1890780\n",
      " Epoch [109/150], Loss: 0.1609053,Test_Loss: 0.1933627\n",
      " Epoch [110/150], Loss: 0.1584810,Test_Loss: 0.1780036\n",
      " Epoch [111/150], Loss: 0.1629258,Test_Loss: 0.1815864\n",
      " Epoch [112/150], Loss: 0.1679371,Test_Loss: 0.1802522\n",
      " Epoch [113/150], Loss: 0.1613916,Test_Loss: 0.1873968\n",
      " Epoch [114/150], Loss: 0.1533420,Test_Loss: 0.1791955\n",
      " Epoch [115/150], Loss: 0.1625810,Test_Loss: 0.1869608\n",
      " Epoch [116/150], Loss: 0.1574942,Test_Loss: 0.1817267\n",
      " Epoch [117/150], Loss: 0.1583736,Test_Loss: 0.1866347\n",
      " Epoch [118/150], Loss: 0.1605316,Test_Loss: 0.1993533\n",
      " Epoch [119/150], Loss: 0.1563424,Test_Loss: 0.1973259\n",
      " Epoch [120/150], Loss: 0.1597240,Test_Loss: 0.1844127\n",
      " Epoch [121/150], Loss: 0.1570946,Test_Loss: 0.1900342\n",
      " Epoch [122/150], Loss: 0.1581661,Test_Loss: 0.1875602\n",
      " Epoch [123/150], Loss: 0.1590419,Test_Loss: 0.1919502\n",
      " Epoch [124/150], Loss: 0.1539094,Test_Loss: 0.1831767\n",
      " Epoch [125/150], Loss: 0.1567865,Test_Loss: 0.1873603\n",
      " Epoch [126/150], Loss: 0.1568736,Test_Loss: 0.1880813\n",
      " Epoch [127/150], Loss: 0.1557046,Test_Loss: 0.1811289\n",
      " Epoch [128/150], Loss: 0.1509920,Test_Loss: 0.1895005\n",
      " Epoch [129/150], Loss: 0.1580494,Test_Loss: 0.1888357\n",
      " Epoch [130/150], Loss: 0.1535965,Test_Loss: 0.1790496\n",
      " Epoch [131/150], Loss: 0.1565412,Test_Loss: 0.1829272\n",
      " Epoch [132/150], Loss: 0.1569462,Test_Loss: 0.1909775\n",
      " Epoch [133/150], Loss: 0.1521282,Test_Loss: 0.1993029\n",
      " Epoch [134/150], Loss: 0.1630382,Test_Loss: 0.2107689\n",
      " Epoch [135/150], Loss: 0.1551121,Test_Loss: 0.1911270\n",
      " Epoch [136/150], Loss: 0.1532791,Test_Loss: 0.1894201\n",
      " Epoch [137/150], Loss: 0.1577913,Test_Loss: 0.1836577\n",
      " Epoch [138/150], Loss: 0.1553595,Test_Loss: 0.1779600\n",
      " Epoch [139/150], Loss: 0.1549352,Test_Loss: 0.1825755\n",
      " Epoch [140/150], Loss: 0.1520020,Test_Loss: 0.1836056\n",
      " Epoch [141/150], Loss: 0.1555512,Test_Loss: 0.1959721\n",
      " Epoch [142/150], Loss: 0.1566544,Test_Loss: 0.1833688\n",
      " Epoch [143/150], Loss: 0.1522136,Test_Loss: 0.1898937\n",
      " Epoch [144/150], Loss: 0.1528512,Test_Loss: 0.1827159\n",
      " Epoch [145/150], Loss: 0.1499343,Test_Loss: 0.2001213\n",
      " Epoch [146/150], Loss: 0.1540544,Test_Loss: 0.1956497\n",
      " Epoch [147/150], Loss: 0.1527272,Test_Loss: 0.1808451\n",
      " Epoch [148/150], Loss: 0.1542824,Test_Loss: 0.1867414\n",
      " Epoch [149/150], Loss: 0.1551342,Test_Loss: 0.1819173\n",
      " Epoch [150/150], Loss: 0.1524759,Test_Loss: 0.1765104\n"
     ]
    }
   ],
   "source": [
    "print(\"dataloader\",len(dataloader_test),len(dataloader))\n",
    "h = [64]\n",
    "for decay in [0.1, 1e-5,0]:\n",
    "    model = Neural_Kan(shape = [in_dim,4,2,1], h = [64])\n",
    "    loss = model.fit(dataloader = dataloader,dataloader_test = dataloader_test, epochs=150, lr=1e-3, weight_decay= decay)\n",
    "    plt.plot(loss, label = f\"{decay}\")\n",
    "plt.title(f\"Effect of L2 Reg. on Test Loss (Univariate NN Shape: {h}, 1472 Params, 10 Inputs)\")\n",
    "plt.yscale('log')\n",
    "plt.ylabel(\"Test Loss: Root Mean Squared Error (RMSE)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Use Sequential to define layers\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(10, hidden),   \n",
    "            nn.ReLU(),                  \n",
    "            nn.Linear(hidden, 1),   \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def fit(self, dataloader, dataloader_test, epochs=100, lr=0.001, decay = 1e-3):\n",
    "        # Define loss function (MSE for regression)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Use RAdam optimizer from torchoptimizer library\n",
    "        optimizer = torch.optim.RAdam(self.parameters(), lr=lr, weight_decay=decay)\n",
    "\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Training Phase\n",
    "            self.train()  # Set the model to training mode\n",
    "            running_train_loss = 0.0\n",
    "            for inputs, labels in dataloader:\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_train_loss += loss.item()\n",
    "            avg_train_loss = running_train_loss / len(dataloader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            self.eval() \n",
    "            running_test_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in dataloader_test:\n",
    "                    outputs = self(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    running_test_loss += loss.item()\n",
    "\n",
    "            avg_test_loss = running_test_loss / len(dataloader_test)\n",
    "            test_losses.append(avg_test_loss)\n",
    "\n",
    "            # Print training and test losses for every 100 epochs\n",
    "            #if (epoch + 1) % 100 == 0:\n",
    "            #    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "            #          f'Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "        return test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [64,128,1024, 2048, 4096]\n",
    "#widths = [[4,8],[2,4]]\n",
    "decays = [0.1, 1e-2, 1e-5,0]\n",
    "for width in widths:\n",
    "    plt.figure()\n",
    "    for decay in decays:\n",
    "        model = SimpleNN(in_dim = in_dim, hidden = width)\n",
    "        loss = model.fit(dataloader = dataloader,dataloader_test = dataloader_test, epochs=200, lr=1e-3, decay= decay)\n",
    "        plt.plot(loss, label = f\"{decay}\")\n",
    "    plt.title(f\"L2 reg on test loss shape:{width}\")\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel(\"Test Loss: Root Mean Squared Error (RMSE)\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend()\n",
    "    plt.show(block=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
