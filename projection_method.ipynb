{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniNN(nn.Module):\n",
    "    def __init__(self, hidden,B = 2):\n",
    "        super(UniNN, self).__init__()\n",
    "        self.depth = 1\n",
    "        self.device = \"cpu\"\n",
    "        self.order = float('inf')\n",
    "        self.B = B\n",
    "        #if hidden > 500:\n",
    "        #    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.hidden = hidden       \n",
    "        self.fc1 = nn.Linear(1, self.hidden)\n",
    "        self.B_1 = torch.ones(1)\n",
    "        self.register_parameter(name='fc1_max_layer_norm', param=torch.nn.Parameter(self.B_1))\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        #self.batch_norm = nn.BatchNorm1d(self.hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        #self.activation = nn.Tanh()                         \n",
    "        self.fc2 = nn.Linear(self.hidden, 1)\n",
    "        self.B_2 = torch.ones(1)\n",
    "        self.register_parameter(name='fc2_max_layer_norm', param=torch.nn.Parameter(self.B_2))\n",
    "        #print(self.B_1, self.B_2)\n",
    "        self.test_loss_reached = False\n",
    "        self.end_test_loss = 0         \n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = self.fc1(x.to(self.device))\n",
    "        out_drop = self.dropout(out_1)\n",
    "        out_act = self.activation(out_drop)\n",
    "        return self.fc2(out_act)\n",
    "\n",
    "    def get_dataloader(self,f,num_samples=5000, batch_size = 32):\n",
    "        X = torch.vstack((torch.rand(num_samples, 1), torch.zeros(num_samples //20 ,1)))\n",
    "        train_dataset = torch.utils.data.TensorDataset(X, f(X))\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        return train_dataloader\n",
    "\n",
    "    def fit(self, dataloader, dataloader_test, epochs=100, lr=0.001, decay = 1e-3, B = 10):\n",
    "        self.to(self.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.RAdam([\n",
    "            {'params': self.fc1.weight},  # Only fc1 weights\n",
    "            {'params': self.fc1.bias},   # Only fc1 biases\n",
    "            {'params': self.fc2.weight},  # Only fc2 weights\n",
    "            {'params': self.fc2.bias},   # Only fc2 biases,  # Only fc1 weights,   # Only fc1 biases\n",
    "        ], lr=lr)\n",
    "        #optimizer_b = optim.RAdam([\n",
    "        #], lr=1e-5)\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        iters = 0\n",
    "        epoch = 0\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=333, gamma=0.1)\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            self.train()\n",
    "            running_train_loss = 0.0\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                overall_loss = loss + (dict(self.named_parameters())[f'fc1_max_layer_norm'].data**2 + dict(self.named_parameters())[f'fc2_max_layer_norm'].data**2).item() * 0.001\n",
    "                optimizer.zero_grad()\n",
    "                #self.project_B()\n",
    "                running_train_loss += loss.item()\n",
    "                self.compute_lipschitz_constant()\n",
    "                overall_loss.backward()\n",
    "                self.compute_grad_B()\n",
    "                self.update_B()\n",
    "                optimizer.step()\n",
    "                self.project_B()\n",
    "                self.project_W()\n",
    "                    \n",
    "            avg_train_loss = running_train_loss / len(dataloader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            self.eval() \n",
    "            running_test_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in dataloader_test:\n",
    "                    outputs = self(inputs.to(self.device))\n",
    "                    loss = criterion(outputs, labels.to(self.device))\n",
    "                    running_test_loss += loss.item()\n",
    "                    \n",
    "            avg_test_loss = running_test_loss / len(dataloader_test)\n",
    "            test_losses.append(avg_test_loss)\n",
    "            if avg_test_loss < 5e-3:\n",
    "                iters += 1\n",
    "            else:\n",
    "                iters = 0\n",
    "            if iters == 10:\n",
    "                self.test_loss_reached = True\n",
    "            scheduler.step()\n",
    "            if epoch > 100:\n",
    "                break\n",
    "        self.end_test_loss = sum(test_losses[-5:]) / 5\n",
    "        self.end_train_loss = sum(train_losses[-5:]) / 5  \n",
    "        self.model_err_sup_norm()\n",
    "        print(self.hidden, B,self.L, self.test_loss_reached, test_losses[-5:])\n",
    "        \n",
    "        return test_losses\n",
    "    \n",
    "    def update_B(self):\n",
    "        #print(self.L)\n",
    "        #print(\"fc1:\", self.fc1_max_layer_norm.data, self.fc2_max_layer_norm.data, \"before\")\n",
    "        self.fc1_max_layer_norm.data = self.fc1_max_layer_norm.data - 0.1 * self.grad_B_1\n",
    "        self.fc2_max_layer_norm.data = self.fc2_max_layer_norm.data - 0.1 * self.grad_B_2\n",
    "        #print(\"fc1:\",self.fc1_max_layer_norm.data, self.fc2_max_layer_norm.data, \"after\")\n",
    "        self.grad_B_1 = 0\n",
    "        self.grad_B_2 = 0\n",
    "\n",
    "    def compute_grad_B(self):\n",
    "        self.grad_B_1 = torch.sum(self.fc1.weight.grad * self.fc1.weight.data / torch.linalg.matrix_norm(self.fc1.weight.data, ord = self.order)) + 2 * 0.001 * self.fc1_max_layer_norm.data ## Reg\n",
    "        self.grad_B_2 = torch.sum(self.fc2.weight.grad * self.fc2.weight.data / torch.linalg.matrix_norm(self.fc2.weight.data, ord = self.order)) + 2 * 0.001 * self.fc1_max_layer_norm.data ## Reg\n",
    "\n",
    "    def project_W(self):\n",
    "        self.fc1.weight.data *= self.fc1_max_layer_norm / torch.linalg.matrix_norm(self.fc1.weight.data, ord = self.order)\n",
    "        self.fc2.weight.data *= self.fc2_max_layer_norm / torch.linalg.matrix_norm(self.fc2.weight.data, ord = self.order)\n",
    "\n",
    "    def project_B(self):\n",
    "        return\n",
    "    # Retrieve B1 and B2 from the parameters\n",
    "        B1 = torch.clamp(self.fc1_max_layer_norm.data, min=1e-6, max=self.B)\n",
    "        B2 = torch.clamp(self.fc2_max_layer_norm.data, min=1e-6, max=self.B)\n",
    "\n",
    "        # Compute the product of B1 and B2\n",
    "        #product = B1 * B2\n",
    "\n",
    "        # If product is already equal to self.B, no need to adjust\n",
    "        #if torch.isclose(product, torch.tensor(self.B, dtype=torch.float32, device=product.device)):\n",
    "        #    return\n",
    "\n",
    "        # Ensure product B1 * B2 = self.B by normalizing one of the values\n",
    "        print(B1 , B2)\n",
    "        if B1 < B2:\n",
    "            # Normalize B1 such that the product B1 * B2 = self.B\n",
    "            B1 = self.B / B2\n",
    "        else:\n",
    "            # Normalize B2 such that the product B1 * B2 = self.B\n",
    "            B2 = self.B / B1\n",
    "\n",
    "        # Clamp the normalized B1 and B2 to stay within [1e-6, self.B]\n",
    "        B1 = torch.clamp(B1, min=1e-6, max=self.B)\n",
    "        B2 = torch.clamp(B2, min=1e-6, max=self.B)\n",
    "\n",
    "        # Reassign the updated values back to the parameters\n",
    "        self.fc1_max_layer_norm.data = B1\n",
    "        self.fc2_max_layer_norm.data = B2\n",
    "        print(self.fc1_max_layer_norm.data, self.fc2_max_layer_norm.data)\n",
    "\n",
    "    def plot_model(self, f, title):\n",
    "        x_train = torch.linspace(0,1,1000)\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(x_train.view(-1,1))\n",
    "        print(\"estimated_sup_norm_error\", torch.max(torch.abs(y_pred.view(-1) - f(x_train))))\n",
    "        plt.plot(x_train.numpy(), y_pred.numpy(), label=\"Model\")\n",
    "        plt.plot(x_train.numpy(), f(x_train).numpy(), label=\"Objective\")\n",
    "        plt.title(\"Model Predictions vs Data\" + ' decay:' + title)\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def reg(self):\n",
    "        reg_loss = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss += torch.linalg.matrix_norm(param, ord = 1)\n",
    "        return reg_loss\n",
    "\n",
    "    def model_err_sup_norm(self):\n",
    "        x_train = torch.linspace(0,1,1000)\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(x_train.view(-1,1)).to(\"cpu\")\n",
    "        self.sup_err = torch.max(torch.abs(y_pred.view(-1) - f(x_train)))\n",
    "\n",
    "    def compute_lipschitz_constant(self):\n",
    "        self.L = 1\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                self.L *= torch.linalg.matrix_norm(param, ord = self.order)\n",
    "                #self.L *= spectral_norm(param)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return X**0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\projection_method.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/projection_method.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m B \u001b[39min\u001b[39;00m Bs:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/projection_method.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     model \u001b[39m=\u001b[39m UniNN(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mi, B \u001b[39m=\u001b[39m B)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/projection_method.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(lr \u001b[39m=\u001b[39;49m \u001b[39m1e-3\u001b[39;49m, dataloader\u001b[39m=\u001b[39;49mdataloader_train, dataloader_test\u001b[39m=\u001b[39;49mdataloader_test, decay\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, B \u001b[39m=\u001b[39;49m B)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/projection_method.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     diction[\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mi]\u001b[39m.\u001b[39mappend(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/projection_method.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m models\u001b[39m.\u001b[39mupdate(diction)\n",
      "\u001b[1;32mc:\\Users\\prass\\Documents\\Code\\KAN\\KAN_Test\\projection_method.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/projection_method.ipynb#W2sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/projection_method.ipynb#W2sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m overall_loss \u001b[39m=\u001b[39m loss \u001b[39m+\u001b[39m (\u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_parameters())[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfc1_max_layer_norm\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdata\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_parameters())[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfc2_max_layer_norm\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdata\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/projection_method.ipynb#W2sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/projection_method.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m#self.project_B()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prass/Documents/Code/KAN/KAN_Test/projection_method.ipynb#W2sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m running_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49mdisable(fn, recursive)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\decorators.py:46\u001b[0m, in \u001b[0;36mdisable\u001b[1;34m(fn, recursive)\u001b[0m\n\u001b[0;32m     44\u001b[0m         fn \u001b[39m=\u001b[39m innermost_fn(fn)\n\u001b[0;32m     45\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(fn)\n\u001b[1;32m---> 46\u001b[0m         \u001b[39mreturn\u001b[39;00m DisableContext()(fn)\n\u001b[0;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m DisableContext()\n\u001b[0;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:437\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(fn)\n\u001b[0;32m    436\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 437\u001b[0m     filename \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mgetsourcefile(fn)\n\u001b[0;32m    438\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:949\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39many\u001b[39m(filename\u001b[39m.\u001b[39mendswith(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m\n\u001b[0;32m    947\u001b[0m              importlib\u001b[39m.\u001b[39mmachinery\u001b[39m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[0;32m    948\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 949\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mexists(filename):\n\u001b[0;32m    950\u001b[0m     \u001b[39mreturn\u001b[39;00m filename\n\u001b[0;32m    951\u001b[0m \u001b[39m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = UniNN(16)\n",
    "dataloader_train = model.get_dataloader(f)\n",
    "dataloader_test = model.get_dataloader(f, num_samples=200)\n",
    "models = {}\n",
    "Bs = [2,9] \n",
    "#Bs = [2]\n",
    "#decays = [1e-2]\n",
    "for i in range(3,4):\n",
    "    diction = {2**i: []}\n",
    "    for B in Bs:\n",
    "        model = UniNN(2**i, B = B)\n",
    "        loss = model.fit(lr = 1e-3, dataloader=dataloader_train, dataloader_test=dataloader_test, decay=0, B = B)\n",
    "        diction[2**i].append(model)\n",
    "    models.update(diction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
